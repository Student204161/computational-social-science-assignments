{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The central idea behind the project is to research communities on a subset of StackOverFlow Q&A using a network describing interactions between users. It is interesting to answer questions like- What is the modularity of the Network, How discernable are the communities from each other in terms of topics discussed & How do they differ? A hypothesis could be that each community uses primarily their own programming language. We could also google the top 5 authors by score or amount of activity to see their specialization - It can also be interesting to look at score, does the highest scoring post depend on community size and to what degree if? Are some communities better at resolving questions? Look at proportion of closed to non-closed questions.\n",
    "\n",
    "The dataset we use for this is 10% of StackOverflow Q&A version 2 from 2019 & contains 2~ million~ questions & 1.2~ million answers. Also write features in both in powerpoint.\n",
    "\n",
    "Dataset is licensed under CC-BY-SA 3.0 with attribution required - meaning it can freely be modified & used both for commercial and research purposes as long as attributed. Top contributors are Miljan Stojiljkovic, Niyamat Ullah, Kartik Garg - See link for rest of contributors.\n",
    "https://www.kaggle.com/datasets/stackoverflow/stacksample\n",
    "\n",
    "\n",
    "Outline:\n",
    "    * Download Dataset. - Done\n",
    "    * Clean out rows with NaN (Except if the NaN is because a question is still open) and clean out stopwords, links, line breaks, etc.- Done\n",
    "    * DataScrape to add UserNames/Names using OwnerUserId, requests & stackexchange API: https://api.stackexchange.com/docs\n",
    "        PS! Since bandwith/quota is not very little compared to dataset size, only top author names are found instead of whole dataset.\n",
    "        \n",
    "    * Make Directed Network with Networkx and calculate best community split using Louvain. Nodes are persons and an edge from one person to another means that the first person replies to a question written by the other person. If multiple replies to same question, count this and add both as attribute.\n",
    "    * WordCloud for best community splits + other diagnostics to analyse social network.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading and Removing NA Rows. Is important to not use for loop, when a solution of constant time complexity exists, when handling data.\n",
    "## Roll down - Don't run the code in first section, takes too much time. Roll down until clean data is loaded & a subset_dataset is made.\n",
    "import numpy as np\n",
    "import threading \n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import networkx as nx\n",
    "from ast import literal_eval\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answers = '/work3/s204161/comp_social_science_data/Answers.csv'\n",
    "questions = '/work3/s204161/comp_social_science_data/Questions.csv'\n",
    "tags = '/work3/s204161/comp_social_science_data/Tags.csv'\n",
    "\n",
    "answers_df = pd.read_csv(answers, encoding='ISO-8859-1')\n",
    "questions_df = pd.read_csv(questions, encoding='ISO-8859-1')\n",
    "tags_df = pd.read_csv(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Answers_df\nSize before: 2014516\nSize After: 2001316\nQuestions_df\nSize before: 1264216\nSize After: 1249762\n"
     ]
    }
   ],
   "source": [
    "#clean rows with NA values in them.\n",
    "clean_answers_df = answers_df.dropna(subset=answers_df.columns, axis=0, how='any')\n",
    "clean_questions_df = questions_df.dropna(subset=questions_df.columns.difference(['ClosedDate']), axis=0, how='any')\n",
    "print(f'Answers_df\\nSize before: {len(answers_df.index)}\\nSize After: {len(clean_answers_df.index)}')\n",
    "print(f'Questions_df\\nSize before: {len(questions_df.index)}\\nSize After: {len(clean_questions_df.index)}')\n",
    "\n",
    "clean_questions_df['OwnerUserId'] = np.asarray(clean_questions_df.get('OwnerUserId'),dtype=int)\n",
    "clean_answers_df['OwnerUserId'] = np.asarray(clean_answers_df.get('OwnerUserId'),dtype=int)\n",
    "\n",
    "clean_answers_df.to_csv('/work3/s204161/comp_social_science_data/no_NA_Answers.csv', encoding='ISO-8859-1', index=False)\n",
    "clean_questions_df.to_csv('/work3/s204161/comp_social_science_data/no_NA_Questions.csv', encoding='ISO-8859-1', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               Id  OwnerUserId          CreationDate  ParentId  Score  \\\n",
       "0              92           61  2008-08-01T14:45:37Z        90     13   \n",
       "1             124           26  2008-08-01T16:09:47Z        80     12   \n",
       "2             199           50  2008-08-01T19:36:46Z       180      1   \n",
       "3             269           91  2008-08-01T23:49:57Z       260      4   \n",
       "4             307           49  2008-08-02T01:49:46Z       260     28   \n",
       "...           ...          ...                   ...       ...    ...   \n",
       "2014511  40143247       333403  2016-10-19T23:42:35Z  40143190      0   \n",
       "2014512  40143322       642706  2016-10-19T23:50:35Z  40137110      1   \n",
       "2014513  40143336      2239781  2016-10-19T23:52:08Z  40141860      0   \n",
       "2014514  40143349      6934347  2016-10-19T23:54:02Z  40077010      0   \n",
       "2014515  40143389      4464432  2016-10-19T23:58:58Z  40142910      0   \n",
       "\n",
       "                                                      Body  \n",
       "0        <p><a href=\"http://svnbook.red-bean.com/\">Vers...  \n",
       "1        <p>I wound up using this. It is a kind of a ha...  \n",
       "2        <p>I've read somewhere the human eye can't dis...  \n",
       "3        <p>Yes, I thought about that, but I soon figur...  \n",
       "4        <p><a href=\"http://www.codeproject.com/Article...  \n",
       "...                                                    ...  \n",
       "2014511  <p>Tanks to <a href=\"http://stackoverflow.com/...  \n",
       "2014512  <h1>tl;dr</h1>\\n\\n<pre><code>ZonedDateTime.par...  \n",
       "2014513  <p>I came up with a very dirty workaround. Bef...  \n",
       "2014514  <p>I solved my own problem defining the follow...  \n",
       "2014515  <p>Try add <code>retrun false</code> in the <c...  \n",
       "\n",
       "[2001316 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>OwnerUserId</th>\n      <th>CreationDate</th>\n      <th>ParentId</th>\n      <th>Score</th>\n      <th>Body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>92</td>\n      <td>61</td>\n      <td>2008-08-01T14:45:37Z</td>\n      <td>90</td>\n      <td>13</td>\n      <td>&lt;p&gt;&lt;a href=\"http://svnbook.red-bean.com/\"&gt;Vers...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>124</td>\n      <td>26</td>\n      <td>2008-08-01T16:09:47Z</td>\n      <td>80</td>\n      <td>12</td>\n      <td>&lt;p&gt;I wound up using this. It is a kind of a ha...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>199</td>\n      <td>50</td>\n      <td>2008-08-01T19:36:46Z</td>\n      <td>180</td>\n      <td>1</td>\n      <td>&lt;p&gt;I've read somewhere the human eye can't dis...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>269</td>\n      <td>91</td>\n      <td>2008-08-01T23:49:57Z</td>\n      <td>260</td>\n      <td>4</td>\n      <td>&lt;p&gt;Yes, I thought about that, but I soon figur...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>307</td>\n      <td>49</td>\n      <td>2008-08-02T01:49:46Z</td>\n      <td>260</td>\n      <td>28</td>\n      <td>&lt;p&gt;&lt;a href=\"http://www.codeproject.com/Article...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2014511</th>\n      <td>40143247</td>\n      <td>333403</td>\n      <td>2016-10-19T23:42:35Z</td>\n      <td>40143190</td>\n      <td>0</td>\n      <td>&lt;p&gt;Tanks to &lt;a href=\"http://stackoverflow.com/...</td>\n    </tr>\n    <tr>\n      <th>2014512</th>\n      <td>40143322</td>\n      <td>642706</td>\n      <td>2016-10-19T23:50:35Z</td>\n      <td>40137110</td>\n      <td>1</td>\n      <td>&lt;h1&gt;tl;dr&lt;/h1&gt;\\n\\n&lt;pre&gt;&lt;code&gt;ZonedDateTime.par...</td>\n    </tr>\n    <tr>\n      <th>2014513</th>\n      <td>40143336</td>\n      <td>2239781</td>\n      <td>2016-10-19T23:52:08Z</td>\n      <td>40141860</td>\n      <td>0</td>\n      <td>&lt;p&gt;I came up with a very dirty workaround. Bef...</td>\n    </tr>\n    <tr>\n      <th>2014514</th>\n      <td>40143349</td>\n      <td>6934347</td>\n      <td>2016-10-19T23:54:02Z</td>\n      <td>40077010</td>\n      <td>0</td>\n      <td>&lt;p&gt;I solved my own problem defining the follow...</td>\n    </tr>\n    <tr>\n      <th>2014515</th>\n      <td>40143389</td>\n      <td>4464432</td>\n      <td>2016-10-19T23:58:58Z</td>\n      <td>40142910</td>\n      <td>0</td>\n      <td>&lt;p&gt;Try add &lt;code&gt;retrun false&lt;/code&gt; in the &lt;c...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2001316 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "clean_answers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "np.sort(clean_answers_df['ParentId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "unique_nodes: 1980582\n"
     ]
    }
   ],
   "source": [
    "unique_users = np.unique(np.asarray(np.append(np.append(clean_questions_df.get('OwnerUserId'),clean_answers_df.get('OwnerUserId')),clean_answers_df.get('ParentId')),dtype=int)) ## This number is wrong, parentId should not be included.\n",
    "print(f'unique_nodes: {len(unique_users)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0              177.0\n",
       "1              164.0\n",
       "2              313.0\n",
       "3          2090091.0\n",
       "4              400.0\n",
       "             ...    \n",
       "2014511          NaN\n",
       "2014512          NaN\n",
       "2014513          NaN\n",
       "2014514          NaN\n",
       "2014515          NaN\n",
       "Length: 2011725, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "np.append(clean_questions_df.get('OwnerUserId'),clean_answers_df.get('OwnerUserId')),clean_answers_df.get('ParentId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([      26,       58,       83, ..., 40141860, 40077010, 40142910])"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "np.append(np.append(clean_questions_df.get('OwnerUserId'),clean_answers_df.get('OwnerUserId')),clean_answers_df.get('ParentId'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(unique_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1000\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m remove_indexes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens)):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#remove bad tokens\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([(tokens[i] \u001b[38;5;241m==\u001b[39m bad_token) \u001b[38;5;28;01mfor\u001b[39;00m bad_token \u001b[38;5;129;01min\u001b[39;00m bad_tokens]):\n\u001b[1;32m     23\u001b[0m         remove_indexes\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m([(bad_token \u001b[38;5;129;01min\u001b[39;00m tokens[i]) \u001b[38;5;28;01mfor\u001b[39;00m bad_token \u001b[38;5;129;01min\u001b[39;00m bad_tokens_2]):\n",
      "Cell \u001b[0;32mIn[45], line 22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m remove_indexes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens)):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#remove bad tokens\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([(tokens[i] \u001b[38;5;241m==\u001b[39m bad_token) \u001b[38;5;28;01mfor\u001b[39;00m bad_token \u001b[38;5;129;01min\u001b[39;00m bad_tokens]):\n\u001b[1;32m     23\u001b[0m         remove_indexes\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m([(bad_token \u001b[38;5;129;01min\u001b[39;00m tokens[i]) \u001b[38;5;28;01mfor\u001b[39;00m bad_token \u001b[38;5;129;01min\u001b[39;00m bad_tokens_2]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "tok_list = []\n",
    "bad_tokens = set(['https:', 'http:', 'www.']) | set(nltk.corpus.stopwords.words('english')) | set(string.punctuation)\n",
    "bad_tokens_2 = set(string.punctuation) | set(['[',']','(',')','{','}','<','>','/','\\\\','|',')',']'])\n",
    "\n",
    "for i in range(len(clean_questions_df.get('OwnerUserId'))):\n",
    "    body_text = clean_questions_df.iloc[i]['Body']\n",
    "    clean_body_text = BeautifulSoup(body_text, 'html.parser').get_text()\n",
    "    #print(clean_body_text) 321\n",
    "    if i % 1000 == 0:\n",
    "       print(i)\n",
    "    tokens = nltk.word_tokenize(clean_body_text)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    remove_indexes = []\n",
    "    for i in range(len(tokens)):\n",
    "        #remove bad tokens\n",
    "        if any([(tokens[i] == bad_token) for bad_token in bad_tokens]):\n",
    "            remove_indexes.append(i)\n",
    "        elif any([(bad_token in tokens[i]) for bad_token in bad_tokens_2]):\n",
    "            remove_indexes.append(i)\n",
    "        elif tokens[i].isnumeric():\n",
    "            remove_indexes.append(i)            \n",
    "    \n",
    "    for i in range(len(remove_indexes) - 1, -1, -1):\n",
    "        remove_index = remove_indexes[i]\n",
    "        del tokens[remove_index]\n",
    "\n",
    "    if len(tokens) == 1:\n",
    "        tokens.remove(tokens[0])\n",
    "\n",
    "        #Maybe do biagrams, if not too computationally demanding, to get some temporal context\n",
    "    tok_list.append(tokens)\n",
    "\n",
    "\n",
    "clean_questions_df['tokens'] = tok_list\n",
    "\n",
    "clean_questions_df.to_csv('/work3/s204161/comp_social_science_data/with_token_Questions.csv', encoding='ISO-8859-1', index=False)\n",
    "\n",
    "#This takes some time to run, so is interrupted and ran overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n",
      "600000\n",
      "650000\n",
      "700000\n",
      "750000\n",
      "800000\n",
      "850000\n",
      "900000\n",
      "950000\n",
      "1000000\n",
      "1050000\n",
      "1100000\n",
      "1150000\n",
      "1200000\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "owner_attrs = {}\n",
    "\n",
    "owner_found_bool = {}\n",
    "#Add question/owner attributes to nodes:\n",
    "for i in range(len(clean_questions_df.get('OwnerUserId'))):\n",
    "    question_id = str(int(clean_questions_df.iloc[i].get('Id')))\n",
    "    owner_id = str(int(clean_questions_df.iloc[i].get('OwnerUserId')))\n",
    "    if not owner_found_bool.get(owner_id):\n",
    "        owner_found_bool[owner_id] = True\n",
    "        owner_attrs[owner_id] = {}\n",
    "        owner_attrs[owner_id]['scores'] = {}\n",
    "    owner_attrs[owner_id][question_id] = BeautifulSoup(clean_questions_df.iloc[i].get('Body'), 'html.parser').get_text()\n",
    "    owner_attrs[owner_id]['scores'][question_id] = (clean_questions_df.iloc[i].get('Score'))    \n",
    "    #Should also save question title in graph - implement here:\n",
    "    \n",
    "\n",
    "    if i % 50000 == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n",
      "600000\n",
      "650000\n",
      "700000\n",
      "750000\n",
      "800000\n",
      "850000\n",
      "900000\n",
      "950000\n",
      "1000000\n",
      "1050000\n",
      "1100000\n",
      "1150000\n",
      "1200000\n",
      "1250000\n",
      "1300000\n",
      "1350000\n",
      "1400000\n",
      "1450000\n",
      "1500000\n",
      "1550000\n",
      "1600000\n",
      "1650000\n",
      "1700000\n",
      "1750000\n",
      "1800000\n",
      "1850000\n",
      "1900000\n",
      "1950000\n",
      "2000000\n"
     ]
    }
   ],
   "source": [
    "answers_attrs = {}\n",
    "answers_found_bool = {}\n",
    "\n",
    "collab_network = {}\n",
    "\n",
    "#Add answer attributes to nodes & remember that edge is directed from OwnerUserId --> ParentId:\n",
    "for i in range(len(clean_answers_df.get('OwnerUserId'))):\n",
    "    answers_id = str(int(clean_answers_df.iloc[i].get('Id')))\n",
    "    owner_id = str(int(clean_answers_df.iloc[i].get('OwnerUserId')))\n",
    "    parent_id = str(int(clean_answers_df.iloc[i].get('ParentId')))\n",
    "    if not answers_found_bool.get(owner_id):\n",
    "        answers_found_bool[owner_id] = True\n",
    "        answers_attrs[owner_id] = {}\n",
    "    if not owner_found_bool.get(owner_id):\n",
    "        owner_found_bool[owner_id] = True\n",
    "        owner_attrs[owner_id] = {}\n",
    "        owner_attrs[owner_id]['scores'] = {}\n",
    "\n",
    "    owner_attrs[owner_id][answers_id] = BeautifulSoup(clean_answers_df.iloc[i].get('Body'), 'html.parser').get_text()\n",
    "    \n",
    "    if collab_network.get((owner_id, parent_id)) == None:\n",
    "        collab_network[(owner_id, parent_id)] = 1\n",
    "    else:\n",
    "        collab_network[(owner_id, parent_id)] += 1\n",
    "\n",
    "    #we want to find scores for each person for answers too.\n",
    "    owner_attrs[owner_id]['scores'][answers_id] = clean_answers_df.iloc[i].get('Score')\n",
    "    if i % 50000 == 0:\n",
    "        print(i)\n",
    "\n",
    "nx.set_node_attributes(G, answers_attrs, 'Questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for owner_id in unique_user_ids:\n",
    "    if owner_attrs.get(owner_id):\n",
    "        owner_attrs[owner_id]['best_score'] = np.max(list(owner_attrs[owner_id]['scores'].values()))\n",
    "        owner_attrs[owner_id]['avg_score'] = np.mean(np.asarray(list(owner_attrs[owner_id]['scores'].values()),dtype=float))\n",
    "        owner_attrs[owner_id]['median_score'] = np.median(list(owner_attrs[owner_id]['scores'].values()))\n",
    "\n",
    "\n",
    "nx.set_node_attributes(G, owner_attrs, 'Questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make edgelist and then edges in network.\n",
    "edgelist = [(x[0][0], x[0][1], x[1]) for x in collab_network.items()]\n",
    "\n",
    "G.add_weighted_edges_from(edgelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#Save Graph\n",
    "with open('/work3/s204161/comp_social_science_data/stackoverflow_network.pickle', 'wb') as f:\n",
    "    pickle.dump(G, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#Graph is big, so check Google Drive Link instead for both social network and csv files: \n",
    "# https://drive.google.com/drive/folders/11gTEA4omR2T6JZRMFCww7e2B1DcoqpmO?usp=share_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of nodes in G: 30133\nNumber of edges in G: 80670\nEdges for node 61 in G: [('81', '71422')]\nEdges for node 61 in G: [('61', '61')]\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of nodes in G: {G.number_of_nodes()}')\n",
    "print(f'Number of edges in G: {G.number_of_edges()}')\n",
    "print(f'Edges for node 61 in G: {G.edges(str(81))}')\n",
    "print(f'Edges for node 61 in G: {G.edges(str(61))}') # This user answers their own questions (and prolly closes the thread)\n",
    "\n",
    "#print(f'Info about Node 61 in G: {G.nodes[str(61)]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below would find username for all users in dataset. Due to Quotas it is not possible however, so after we find top authors in our best community-split, we can request some of the top author names using stackexchange API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "break for test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Since bandwith/quota is not very little compared to dataset size, only top author names are found instead of whole dataset. This code should be applied on top authors only then.\n",
    "import requests\n",
    "import time\n",
    "remaining_users = []\n",
    "username_dict = {}\n",
    "for user_index in range(0,len(unique_user_ids),100):\n",
    "    unsuccesful_tries = 0\n",
    "    url = f'https://api.stackexchange.com/2.3/users/{\";\".join(map(str, unique_user_ids[user_index:user_index + 100]))}?site=meta.stackoverflow'\n",
    "    #url = f'https://api.stackexchange.com/2.3/users/{61}?site=meta.stackoverflow'\n",
    "    response = requests.get(url)\n",
    "    while response.status_code != 200:\n",
    "        print(f'Failed with status code: {response.status_code}')\n",
    "        time.sleep(5)\n",
    "        if unsuccesful_tries == 3:\n",
    "            remaining_users.append(unique_user_ids[user_index:user_index + 100])\n",
    "            continue\n",
    "\n",
    "    #data = response.json()['items'][0]\n",
    "    #username_dict[str(user)] = data['display_name']\n",
    "    print('break for test')\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "source": [
    "set1 = set(np.asarray(clean_answers_df.get('Id'),dtype=str))\n",
    "set2 = set(np.asarray(clean_questions_df.get('Id'),dtype=str))\n",
    "#Id is unique in whole dataset/on stackoverflow page - is not shared between answers & questions. thats why intersection is empty\n",
    "set1.intersection(set2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "#Save Graph\n",
    "with open('/work3/s204161/comp_social_science_data/stackoverflow_network.pickle', 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "#Graph is big, so check Google Drive Link instead for both social network and csv files: \n",
    "# https://drive.google.com/drive/folders/11gTEA4omR2T6JZRMFCww7e2B1DcoqpmO?usp=share_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seen_nodes = {}\n",
    "\n",
    "for node in G.nodes:\n",
    "    if seen_nodes.get(int(node)):\n",
    "        print('fucker is seen >:-(', node)\n",
    "    seen_nodes[node] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# NEW CODE STARTS HERE using full tokenized answers & questions to make subset_data\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "answers = '/work3/s204161/comp_social_science_data/with_token_Answers.csv'\n",
    "questions = '/work3/s204161/comp_social_science_data/with_token_Questions.csv'\n",
    "questions_df = pd.read_csv(questions, encoding='utf-8')\n",
    "answers_df = pd.read_csv(answers, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df.tokens = answers_df.tokens.apply(lambda x: literal_eval(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset by Time\n",
    "subset_questions_df = questions_df[questions_df.CreationDate.str[:4].astype('int') == 2009]\n",
    "subset_answers_df = answers_df[(2010 > answers_df.CreationDate.str[:4].astype('int')) & (answers_df.CreationDate.str[:4].astype('int') >= 2009)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dict to ensure linear time instead of quadratic\n",
    "subset_question_dict = {}\n",
    "for ques in list(subset_questions_df.Id):\n",
    "    subset_question_dict[ques] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 92887/92887 [01:36<00:00, 959.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "for ans_index in tqdm.tqdm(range(len(subset_answers_df)-1, -1,-1)):\n",
    "    parid = subset_answers_df.iloc[ans_index].ParentId\n",
    "    if not subset_question_dict.get(parid):\n",
    "        subset_answers_df.drop(subset_answers_df.index[ans_index], inplace=True)\n",
    "        #question_dict.get(subset_answers_df.iloc[ans_index].ParentId)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "question_df_length: 31088\n",
      "answers_df_length: 92887\n",
      "answers_df_length_after_removing answers to questions from 2008 or previous years: 82497\n"
     ]
    }
   ],
   "source": [
    "print('question_df_length:',len(subset_questions_df))\n",
    "print('answers_df_length:',len(answers_df[(2010 > answers_df.CreationDate.str[:4].astype('int')) & (answers_df.CreationDate.str[:4].astype('int') >= 2009)]))\n",
    "print('answers_df_length_after_removing answers to questions from 2008 or previous years:', len(subset_answers_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "unique_nodes: 30133\n"
     ]
    }
   ],
   "source": [
    "unique_users = np.unique(np.asarray(np.asarray(np.append(subset_questions_df.get('OwnerUserId'),subset_answers_df.get('OwnerUserId')),dtype=int),dtype=str))\n",
    "print(f'unique_nodes: {len(unique_users)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "def tokenize_col(dat_frame_df,colname, calculate_bigrams):\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    #tok_list = []\n",
    "    bad_tokens = set(['https:', 'http:', 'www.']) | set(nltk.corpus.stopwords.words('english')) | set(string.punctuation)\n",
    "    bad_tokens_2 = set(string.punctuation) | set(['[',']','(',')','{','}','<','>','/','\\\\','|',')',']'])\n",
    "\n",
    "    tok_list = []\n",
    "    bi_list = []\n",
    "    for i in range(len(dat_frame_df.get('OwnerUserId'))):\n",
    "        body_text = dat_frame_df.iloc[i][colname]\n",
    "        clean_body_text = BeautifulSoup(body_text, 'html.parser').get_text()\n",
    "        #print(clean_body_text) 321\n",
    "        if i % 5000 == 0:\n",
    "            print(i)\n",
    "        tokens = nltk.word_tokenize(clean_body_text)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        remove_indexes = []\n",
    "        for i in range(len(tokens)):\n",
    "            #remove bad tokens\n",
    "            if any([(tokens[i] == bad_token) for bad_token in bad_tokens]):\n",
    "                remove_indexes.append(i)\n",
    "            elif any([(bad_token in tokens[i]) for bad_token in bad_tokens_2]):\n",
    "                remove_indexes.append(i)\n",
    "            elif tokens[i].isnumeric():\n",
    "                remove_indexes.append(i)            \n",
    "        \n",
    "        for i in range(len(remove_indexes) - 1, -1, -1):\n",
    "            remove_index = remove_indexes[i]\n",
    "            del tokens[remove_index]\n",
    "\n",
    "        if len(tokens) == 1:\n",
    "            tokens.remove(tokens[0])\n",
    "            #Maybe do biagrams, if not too computationally demanding, to get temporal context\n",
    "        \n",
    "        if calculate_bigrams:\n",
    "            bi_grams = list(nltk.bigrams(tokens))\n",
    "            bi_list.append(bi_grams)\n",
    "        tok_list.append(tokens)\n",
    "\n",
    "    return tok_list if not calculate_bigrams else tok_list, bi_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_questions_df = subset_questions_df.drop(columns='tokens')\n",
    "#subset_answers_df = subset_answers_df.drop(columns='tokens')\n",
    "#subset_questions_df = subset_questions_df.drop(columns='TitleTokens')\n",
    "#subset_questions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /zhome/a7/0/155527/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /zhome/a7/0/155527/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /zhome/a7/0/155527/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /zhome/a7/0/155527/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /zhome/a7/0/155527/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /zhome/a7/0/155527/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subset_questions_df['TitleTokens'], subset_questions_df['TitleBigrams']  = tokenize_col(subset_questions_df,colname='Title',calculate_bigrams=True)\n",
    "\n",
    "subset_questions_df['Tokens'], subset_questions_df['TokensBigrams']  = tokenize_col(subset_questions_df,colname='Body',calculate_bigrams=True)\n",
    "\n",
    "subset_answers_df['Tokens'], subset_answers_df['TokensBigrams']  = tokenize_col(subset_answers_df,colname='Body',calculate_bigrams=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_questions_df.to_csv('/work3/s204161/comp_social_science_data/subset_questions_c.csv', encoding='utf-8', index=False)\n",
    "subset_answers_df.to_csv('/work3/s204161/comp_social_science_data/subset_answers_c.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            Id  OwnerUserId          CreationDate  ParentId  Score  \\\n",
       "0       404434       1288.0  2009-01-01T02:46:21Z    404430      6   \n",
       "1       404436      20029.0  2009-01-01T02:46:58Z    404430      1   \n",
       "...        ...          ...                   ...       ...    ...   \n",
       "82495  1987478      31668.0  2009-12-31T23:29:25Z   1987470      1   \n",
       "82496  2274531     165358.0  2009-10-26T15:24:46Z   2274530      1   \n",
       "\n",
       "                                                    Body  \\\n",
       "0      <p>The most common use cases are to find strin...   \n",
       "1      <p>Stack Overflow is in fact a good place to f...   \n",
       "...                                                  ...   \n",
       "82495  <p>I would recommend first getting a list of a...   \n",
       "82496  <p>Does this Microsoft Support article help:<b...   \n",
       "\n",
       "                                                  Tokens  \\\n",
       "0      [common, use, cases, find, strings, match, pat...   \n",
       "1      [stack, overflow, fact, good, place, find, use...   \n",
       "...                                                  ...   \n",
       "82495  [would, recommend, first, getting, list, eleme...   \n",
       "82496  [microsoft, support, article, help, migrate, v...   \n",
       "\n",
       "                                           TokensBigrams  \n",
       "0      [(common, use), (use, cases), (cases, find), (...  \n",
       "1      [(stack, overflow), (overflow, fact), (fact, g...  \n",
       "...                                                  ...  \n",
       "82495  [(would, recommend), (recommend, first), (firs...  \n",
       "82496  [(microsoft, support), (support, article), (ar...  \n",
       "\n",
       "[82497 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>OwnerUserId</th>\n      <th>CreationDate</th>\n      <th>ParentId</th>\n      <th>Score</th>\n      <th>Body</th>\n      <th>Tokens</th>\n      <th>TokensBigrams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>404434</td>\n      <td>1288.0</td>\n      <td>2009-01-01T02:46:21Z</td>\n      <td>404430</td>\n      <td>6</td>\n      <td>&lt;p&gt;The most common use cases are to find strin...</td>\n      <td>[common, use, cases, find, strings, match, pat...</td>\n      <td>[(common, use), (use, cases), (cases, find), (...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>404436</td>\n      <td>20029.0</td>\n      <td>2009-01-01T02:46:58Z</td>\n      <td>404430</td>\n      <td>1</td>\n      <td>&lt;p&gt;Stack Overflow is in fact a good place to f...</td>\n      <td>[stack, overflow, fact, good, place, find, use...</td>\n      <td>[(stack, overflow), (overflow, fact), (fact, g...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>82495</th>\n      <td>1987478</td>\n      <td>31668.0</td>\n      <td>2009-12-31T23:29:25Z</td>\n      <td>1987470</td>\n      <td>1</td>\n      <td>&lt;p&gt;I would recommend first getting a list of a...</td>\n      <td>[would, recommend, first, getting, list, eleme...</td>\n      <td>[(would, recommend), (recommend, first), (firs...</td>\n    </tr>\n    <tr>\n      <th>82496</th>\n      <td>2274531</td>\n      <td>165358.0</td>\n      <td>2009-10-26T15:24:46Z</td>\n      <td>2274530</td>\n      <td>1</td>\n      <td>&lt;p&gt;Does this Microsoft Support article help:&lt;b...</td>\n      <td>[microsoft, support, article, help, migrate, v...</td>\n      <td>[(microsoft, support), (support, article), (ar...</td>\n    </tr>\n  </tbody>\n</table>\n<p>82497 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 348
    }
   ],
   "source": [
    "#pd.set_option('display.max_rows', 5)\n",
    "subset_answers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "quest_attrs = {}\n",
    "\n",
    "quest_found_bool = {}\n",
    "#Add question/owner attributes to nodes:\n",
    "for i in range(len(subset_questions_df.get('OwnerUserId'))):\n",
    "    question_id = str(subset_questions_df.iloc[i].get('Id'))\n",
    "    owner_id = str(int(subset_questions_df.iloc[i].get('OwnerUserId')))\n",
    "    #If first time user is asking, create dict\n",
    "    if not quest_found_bool.get(owner_id):\n",
    "        quest_found_bool[owner_id] = True\n",
    "        quest_attrs[owner_id] = {}\n",
    "        quest_attrs[owner_id]['Scores'] = {}\n",
    "    #owner_attrs[owner_id][question_id] = BeautifulSoup(subset_questions_df.iloc[i].get('Body'), 'html.parser').get_text()\n",
    "    quest_attrs[owner_id]['Scores'][question_id] = (subset_questions_df.iloc[i].get('Score'))    \n",
    "    #Should also save question title in graph - implement here:\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "nx.set_node_attributes(G, quest_attrs, 'Questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "answers_attrs = {}\n",
    "answers_found_bool = {}\n",
    "\n",
    "collab_network = {}\n",
    "\n",
    "#Add answer attributes to nodes & remember that edge is directed from OwnerUserId --> OwnerUserId (parent):\n",
    "for i in range(len(subset_answers_df.get('OwnerUserId'))):\n",
    "    answers_id = str(subset_answers_df.iloc[i].get('Id'))\n",
    "    owner_id = str(int(subset_answers_df.iloc[i].get('OwnerUserId')))\n",
    "    post_parent_id = str(int(subset_answers_df.iloc[i].get('ParentId')))\n",
    "    owner_parent_id = str(int(subset_questions_df[subset_questions_df.Id == int(post_parent_id)].OwnerUserId))\n",
    "    #if first time user is answering create dict\n",
    "    if not answers_found_bool.get(owner_id):\n",
    "        answers_found_bool[owner_id] = True\n",
    "        answers_attrs[owner_id] = {}\n",
    "        answers_attrs[owner_id]['Scores'] = {}\n",
    "    #owner_attrs[owner_id][answers_id] = BeautifulSoup(clean_answers_df.iloc[i].get('Body'), 'html.parser').get_text()\n",
    "\n",
    "    if not collab_network.get((owner_id, owner_parent_id)):\n",
    "        collab_network[(owner_id, owner_parent_id)] = 1\n",
    "    else:\n",
    "        collab_network[(owner_id, owner_parent_id)] += 1\n",
    "\n",
    "    #we want to find scores for each person for answers too.\n",
    "    answers_attrs[owner_id]['Scores'][answers_id] = subset_answers_df.iloc[i].get('Score')\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "\n",
    "nx.set_node_attributes(G, answers_attrs, 'Answers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Should be updated if calculate for best, avg. median...\n",
    "# for owner_id in unique_users:\n",
    "#     if owner_attrs.get(owner_id):\n",
    "#         owner_attrs[owner_id]['BestScore'] = np.max(list(owner_attrs[owner_id]['Scores'].values()))\n",
    "#         owner_attrs[owner_id]['AvgScore'] = np.mean(np.asarray(list(owner_attrs[owner_id]['scores'].values()),dtype=float))\n",
    "#         owner_attrs[owner_id]['MedianScore'] = np.median(list(owner_attrs[owner_id]['scores'].values()))\n",
    "\n",
    "\n",
    "# nx.set_node_attributes(G, owner_attrs, 'Questions')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make edgelist and then edges in network.\n",
    "edgelist = [(x[0][0], x[0][1], x[1]) for x in collab_network.items()]\n",
    "\n",
    "G.add_weighted_edges_from(edgelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/work3/s204161/comp_social_science_data/stackoverflow_subset_network.pickle', 'wb') as f:\n",
    "    pickle.dump(G, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "InEdgeDataView([('31641', '4'), ('43219', '4'), ('40650', '4'), ('4', '4'), ('130812', '4')])"
      ]
     },
     "metadata": {},
     "execution_count": 340
    }
   ],
   "source": [
    "G.in_edges('404430')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Answers': {'Scores': {'701489': 0, '701781': 3}}}"
      ]
     },
     "metadata": {},
     "execution_count": 348
    }
   ],
   "source": [
    "G.nodes['85162'] #some users are only active once a year, so subsetting by \"time\" will not encapsulate all posts made by a person who is active throughout many years. maybe do it by tag instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "### After making subset_dataset, we request information about authors in this subset.\n",
    "##################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "subset_questions_df = pd.read_csv('/work3/s204161/comp_social_science_data/subset_questions_c.csv', encoding='utf-8')\n",
    "subset_answers_df = pd.read_csv('/work3/s204161/comp_social_science_data/subset_answers_c.csv', encoding='utf-8')\n",
    "\n",
    "subset_answers_df.Tokens = subset_answers_df.Tokens.apply(lambda x: literal_eval(str(x)))\n",
    "subset_answers_df.TokensBigrams = subset_answers_df.TokensBigrams.apply(lambda x: literal_eval(str(x)))\n",
    "\n",
    "subset_questions_df.Tokens = subset_questions_df.Tokens.apply(lambda x: literal_eval(str(x)))\n",
    "subset_questions_df.TokensBigrams = subset_questions_df.TokensBigrams.apply(lambda x: literal_eval(str(x)))\n",
    "subset_questions_df.TitleTokens = subset_questions_df.TitleTokens.apply(lambda x: literal_eval(str(x)))\n",
    "subset_questions_df.TitleBigrams = subset_questions_df.TitleBigrams.apply(lambda x: literal_eval(str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users = np.unique(np.asarray(np.asarray(np.append(subset_questions_df.get('OwnerUserId'),subset_answers_df.get('OwnerUserId')),dtype=int),dtype=str))\n",
    "print(len(unique_users))\n",
    "first_batch = unique_users[:29400]\n",
    "print(len(first_batch))\n",
    "second_batch = unique_users[29400:]\n",
    "assert len(second_batch) + len(first_batch) == len(unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Since bandwith/quota is not very little compared to dataset size, only top author names are found instead of whole dataset. This code should be applied on top authors only then.\n",
    "import requests\n",
    "import time\n",
    "params = {\n",
    "    'pagesize': 100,\n",
    "    'page': 1,\n",
    "}\n",
    "remaining_users = []\n",
    "all_data = []\n",
    "username_dict = {}\n",
    "for user_index in range(29400,30200,100):\n",
    "    unsuccesful_tries = 0\n",
    "    url = f'https://api.stackexchange.com/2.3/users/{\";\".join(map(str, unique_users[user_index:user_index + 100]))}?site=meta.stackoverflow'\n",
    "    #url = f'https://api.stackexchange.com/2.3/users/{61}?site=meta.stackoverflow'\n",
    "    response = requests.get(url,params = params)\n",
    "    while response.status_code != 200:\n",
    "        print(f'Failed with status code: {response.status_code}')\n",
    "        time.sleep(5)\n",
    "        if unsuccesful_tries == 3:\n",
    "            remaining_users.append(unique_users[user_index:user_index + 100])\n",
    "            continue\n",
    "    time.sleep(3)\n",
    "    if response.json()['has_more']:\n",
    "        print(f'oh no, there was more data not fetched, at {user_index} to {user_index + 100}')\n",
    "    data = response.json()['items']\n",
    "\n",
    "    for data_point in data:\n",
    "        all_data.append(data_point)\n",
    "\n",
    "    #username_dict[str(user)] = data['display_name']\n",
    "    print(user_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17327\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('/work3/s204161/comp_social_science_data/first_part_response_0-29400.json') as f:\n",
    "    part_1 = json.load(f)\n",
    "with open('/work3/s204161/comp_social_science_data/second_part_response_29400-30133.json') as f:\n",
    "    part_2 = json.load(f)\n",
    "\n",
    "response_data = part_1 + part_2\n",
    "print(len(response_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "47 or so of these dont exist, \n ['1' '100' '1000' '100004' '100007' '100008' '100014' '100017' '100020'\n '100027' '10004' '100040' '100043' '100066' '10007' '100073' '100080'\n '100089' '100091' '100095' '100110' '100116' '10012' '100128' '100135'\n '100142' '100146' '100157' '10016' '100160' '100165' '100170' '100171'\n '100175' '10018' '100184' '100186' '100187' '100190' '100192' '100203'\n '100208' '100213' '100217' '100223' '100237' '100238' '100240' '100253'\n '100259' '10026' '100261' '100262' '100265' '100272' '100288' '100297'\n '100300' '100302' '100306' '10031' '100322' '100334' '10034' '100347'\n '100358' '100363' '100385' '10039' '10040' '100408' '100426' '100429'\n '100431' '100443' '100450' '100458' '10046' '100466' '10047' '100472'\n '100473' '100478' '100488' '1005' '100506' '100507' '100516' '100519'\n '100527' '100531' '100534' '100552' '100567' '100572' '100577' '100589'\n '100596' '100598' '100609']\n"
     ]
    }
   ],
   "source": [
    "# Only about half of these users still exist - Their account must have been deleted:\n",
    "### For example:\n",
    "# https://meta.stackoverflow.com/users/100027/ Exists.\n",
    "# https://meta.stackoverflow.com/users/100157/ Does not exist\n",
    "print(f'47 or so of these dont exist, \\n {unique_users[0:100]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Answers': {'Scores': {'1041206': 3,\n",
       "   '1052993': 6,\n",
       "   '1126074': 69,\n",
       "   '1164126': 1,\n",
       "   '1226761': 2}},\n",
       " 'user_name': 'Martijn Pieters',\n",
       " 'user_type': 'moderator',\n",
       " 'location': 'Cambridge, UK',\n",
       " 'reputation_today': 1027591,\n",
       " 'badge_counts_today': {'bronze': 754, 'silver': 922, 'gold': 96}}"
      ]
     },
     "metadata": {},
     "execution_count": 478
    }
   ],
   "source": [
    "# Pull out the values in response data we are interested in.\n",
    "tot_users_attrs = {}\n",
    "for user in response_data:\n",
    "    user_attrs = {}\n",
    "    user_id = str(user['user_id'])\n",
    "    assert isinstance(user_id, str)\n",
    "    user_attrs['user_name'] = user['display_name']\n",
    "    user_attrs['user_type'] = user['user_type']\n",
    "    user_attrs['location'] = user.get('location')\n",
    "    user_attrs['reputation_today'] = user['reputation']\n",
    "    user_attrs['badge_counts_today'] = user['badge_counts']\n",
    "\n",
    "    tot_users_attrs[user_id] = user_attrs\n",
    "\n",
    "#Add responses to network as node attributes.\n",
    "import pickle\n",
    "import community as community_louvain\n",
    "nx.set_node_attributes(G, tot_users_attrs)\n",
    "\n",
    "#save network w. response attrs.\n",
    "with open(r'/work3/s204161/comp_social_science_data/stackoverflow_subset_network.pickle', 'wb') as f:\n",
    "    pickle.dump(G, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "G.nodes['100297'] #example node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "num_communities: 1082\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "import pickle\n",
    "with open(r'/work3/s204161/comp_social_science_data/stackoverflow_subset_network_old.pickle', 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "G_un = G.to_undirected()\n",
    "best_partition = community_louvain.best_partition(G_un)\n",
    "num_communities = len(np.unique(list(best_partition.values()))) #Number of communities\n",
    "print(f'num_communities: {num_communities}')\n",
    "communities = {}\n",
    "for num in range(num_communities):\n",
    "    communities[num] = []\n",
    "\n",
    "for node in best_partition:\n",
    "    communities[best_partition[node]].append(node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = community to extract tokens/bigrams for\n",
    "def extract_community_tokens(communities, c, answers_df, questions_df, collect_answers_and_questions = True, multiply_title_weight = 1):\n",
    "    sema.acquire()\n",
    "    assert isinstance(multiply_title_weight,int)\n",
    "    assert (multiply_title_weight > 0)\n",
    "    answer_tokens, community_questions,community_answers = [],[],[]\n",
    "\n",
    "    for user in communities[c]:\n",
    "\n",
    "        user_questions = G.nodes[user].get('Questions')\n",
    "        user_answers = G.nodes[user].get('Answers')\n",
    "        if user_questions:\n",
    "            user_question_ids = list(user_questions['Scores'].keys())\n",
    "            community_questions += user_question_ids            \n",
    "        if user_answers:\n",
    "            user_answers_ids = list(user_answers['Scores'].keys())\n",
    "            community_answers += user_answers_ids\n",
    "        \n",
    "    answers_tok = subset_answers_df[subset_answers_df['Id'].astype(str).isin(community_answers)].Tokens.tolist() \n",
    "    answers_bi = subset_answers_df[subset_answers_df['Id'].astype(str).isin(community_answers)].TokensBigrams.tolist() \n",
    "    questions_tok = subset_questions_df[subset_questions_df['Id'].astype(str).isin(community_questions)].Tokens.tolist()\n",
    "    questions_bi = subset_questions_df[subset_questions_df['Id'].astype(str).isin(community_questions)].TokensBigrams.tolist()\n",
    "    questions_title_tok = subset_questions_df[subset_questions_df['Id'].astype(str).isin(community_questions)].TitleTokens.tolist()\n",
    "    questions_title_bigrams = subset_questions_df[subset_questions_df['Id'].astype(str).isin(community_questions)].TitleBigrams.tolist()\n",
    "    \n",
    "    answers_tok, answers_bi = [item for sublist in answers_tok for item in sublist], [item for sublist in answers_bi for item in sublist] \n",
    "    questions_tok, questions_bi, questions_title_tok, questions_title_bigrams = [item for sublist in questions_tok for item in sublist], [item for sublist in questions_bi for item in sublist], [item for sublist in questions_title_tok for item in sublist], [item for sublist in questions_title_bigrams for item in sublist] \n",
    "\n",
    "    multiply_title_weight -= 1\n",
    "    while multiply_title_weight > 0:\n",
    "        questions_title_tok.append(questions_title_tok)\n",
    "        questions_title_bigrams.extend(questions_title_bigrams)\n",
    "        multiply_title_weight -= 1\n",
    "\n",
    "    all_tokens, all_bigrams = [],[]\n",
    "\n",
    "    if collect_answers_and_questions:\n",
    "        all_tokens.extend(questions_title_tok)\n",
    "        all_tokens.extend(questions_tok)\n",
    "        all_tokens.extend(answers_tok)\n",
    "        \n",
    "        all_bigrams.extend(questions_title_bigrams)\n",
    "        all_bigrams.extend(questions_bi)\n",
    "        all_bigrams.extend(answers_bi)\n",
    "    else:\n",
    "        questions_title_tok.extend(questions_tok)\n",
    "        all_tokens.append(questions_title_tok)\n",
    "        all_tokens.append(answers_tok)\n",
    "\n",
    "        questions_title_bigrams.extend(questions_bi)\n",
    "        all_bigrams.append(questions_title_bigrams)\n",
    "        all_bigrams.append(answers_bi)\n",
    "\n",
    "    with thread_lock:\n",
    "        tok_arrays[c], bigram_arrays[c] = all_tokens, all_bigrams\n",
    "    sema.release()\n",
    "    return all_tokens, all_bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1045/1045 [02:35<00:00,  6.71it/s]\n",
      "100%|██████████| 1045/1045 [02:36<00:00,  6.69it/s]\n",
      "tok_array returned as [questions, answers]\n",
      "100%|██████████| 1045/1045 [02:38<00:00,  6.57it/s]\n",
      "100%|██████████| 1045/1045 [02:39<00:00,  6.55it/s]\n",
      "tok_array returned as [questions, answers]\n"
     ]
    }
   ],
   "source": [
    "# Info about multiprocessing and threading in python: https://medium.com/python-experiments/parallelising-in-python-mutithreading-and-mutiprocessing-with-practical-templates-c81d593c1c49\n",
    "\n",
    "sema = threading.Semaphore(value=16) #set maxthreads to to avoid out of memory error.\n",
    "\n",
    "tok_arrays, bigram_arrays, thread_list = {}, {}, [] #thread_list used for multithreading\n",
    "thread_lock = threading.Lock()\n",
    "for c in communities:\n",
    "    t = threading.Thread(target=extract_community_tokens, args=(communities, c, subset_answers_df, subset_questions_df,True))\n",
    "    thread_list.append(t)\n",
    "# start the threads\n",
    "for thread in tqdm(thread_list):\n",
    "    thread.start()\n",
    "# wait for all threads to complete\n",
    "for thread in thread_list:\n",
    "    thread.join()\n",
    "\n",
    "community_df = pd.DataFrame([tok_arrays]).T\n",
    "community_df.columns = ['AllTokens']\n",
    "community_df['AllBigrams'] = bigram_arrays\n",
    "\n",
    "#Calculate again, but now using collect_answers_and_questions=False so tokens and bigrams are calculated for answers and questions.\n",
    "tok_arrays, bigram_arrays, thread_list = {}, {}, []\n",
    "thread_lock = threading.Lock()\n",
    "for c in communities:\n",
    "    t = threading.Thread(target=extract_community_tokens, args=(communities, c, subset_answers_df, subset_questions_df,False))\n",
    "    thread_list.append(t)\n",
    "\n",
    "for thread in tqdm(thread_list):\n",
    "    thread.start()\n",
    "for thread in thread_list:\n",
    "    thread.join()\n",
    "\n",
    "print('tok_array returned as [questions, answers]')        \n",
    "community_df['QuestionsTokens'] = [tok_arrays[x][0] for x in tok_arrays]\n",
    "community_df['AnswersTokens'] = [tok_arrays[x][1] for x in tok_arrays]\n",
    "community_df['QuestionsBigrams'] = [bigram_arrays[x][0] for x in bigram_arrays]\n",
    "community_df['AnswersBigrams'] = [bigram_arrays[x][1] for x in bigram_arrays]\n",
    "\n",
    "#Multiply occurences in Question Title with alpha - weight Question Titles higher in TF-IDF.\n",
    "alpha = 3 \n",
    "\n",
    "tok_arrays, bigram_arrays, thread_list = {}, {}, []\n",
    "thread_lock = threading.Lock()\n",
    "for c in communities:\n",
    "    t = threading.Thread(target=extract_community_tokens, args=(communities, c, subset_answers_df, subset_questions_df,True,alpha))\n",
    "    thread_list.append(t)\n",
    "\n",
    "for thread in tqdm(thread_list):\n",
    "    thread.start()\n",
    "for thread in thread_list:\n",
    "    thread.join()\n",
    "\n",
    "\n",
    "community_df_a3 = pd.DataFrame([tok_arrays]).T\n",
    "community_df_a3.columns = ['AllTokens']\n",
    "community_df_a3['AllBigrams'] = bigram_arrays\n",
    "\n",
    "tok_arrays, bigram_arrays, thread_list = {}, {}, []\n",
    "thread_lock = threading.Lock()\n",
    "for c in communities:\n",
    "    t = threading.Thread(target=extract_community_tokens, args=(communities, c, subset_answers_df, subset_questions_df,False,alpha))\n",
    "    thread_list.append(t)\n",
    "\n",
    "for thread in tqdm(thread_list):\n",
    "    thread.start()\n",
    "for thread in thread_list:\n",
    "    thread.join()\n",
    "\n",
    "print('tok_array returned as [questions, answers]')        \n",
    "community_df_a3['QuestionsTokens_alpha_3'] = [tok_arrays[x][0] for x in tok_arrays]\n",
    "community_df_a3['AnswersTokens_alpha_3'] = [tok_arrays[x][1] for x in tok_arrays]\n",
    "community_df_a3['QuestionsBigrams_alpha_3'] = [bigram_arrays[x][0] for x in bigram_arrays]\n",
    "community_df_a3['AnswersBigrams_alpha_3'] = [bigram_arrays[x][1] for x in bigram_arrays]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of members in each community using np.unique()\n",
    "indexes, counts = np.unique(list(best_partition.values()), return_counts=True)\n",
    "community_df['community_size'] = [0]*len(indexes)\n",
    "community_df_a3['community_size'] = [0]*len(indexes)\n",
    "for i in indexes:\n",
    "    community_df['community_size'].loc[i] = counts[i]\n",
    "    community_df_a3['community_size'].loc[i] = counts[i]\n",
    "    \n",
    "\n",
    "community_df.to_csv(r'/work3/s204161/comp_social_science_data/community_df.csv', index = False)\n",
    "community_df_a3.to_csv(r'/work3/s204161/comp_social_science_data/community_df_alpha3.csv', index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1082"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "community_df = pd.read_csv('/work3/s204161/comp_social_science_data/community_df.csv', encoding='utf-8')\n",
    "community_df_a3 = pd.read_csv('/work3/s204161/comp_social_science_data/community_df_alpha3.csv', encoding='utf-8')\n",
    "\n",
    "community_df.AllTokens = community_df.AllTokens.apply(lambda x: literal_eval(str(x)))\n",
    "community_df.AllBigrams = community_df.AllBigrams.apply(lambda x: literal_eval(str(x)))\n",
    "\n",
    "community_df_a3.AllTokens = community_df_a3.AllTokens.apply(lambda x: literal_eval(str(x)))\n",
    "community_df_a3.AllBigrams = community_df_a3.AllBigrams.apply(lambda x: literal_eval(str(x)))\n",
    "\n",
    "len(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              AllTokens  \\\n",
       "0     ['add', 'bottom', 'padding', 'div', 'contains'...   \n",
       "1     ['rectangular', 'arrays', 'access', 'loop', 'r...   \n",
       "2     ['segfault', 'adding', 'variable', 'linq', 'sq...   \n",
       "3     ['make', 'jquery', 'modify', 'one', 'div', 'in...   \n",
       "4     ['xslt', 'buddy', 'available', 'somewhere', 's...   \n",
       "...                                                 ...   \n",
       "1040  ['magento', 'country', 'codes', 'table', 'rate...   \n",
       "1041  ['use', 'panel', 'region', 'prism', 'prism', '...   \n",
       "1042  ['work', 'canvas', 'jcanvas', 'netbeans', 'net...   \n",
       "1043  ['soap', 'formatting', 'problems', 'change', '...   \n",
       "1044  ['use', 'extensibility', 'dll', 'designer', 's...   \n",
       "\n",
       "                                             AllBigrams  \\\n",
       "0     [('add', 'bottom'), ('bottom', 'padding'), ('p...   \n",
       "1     [('rectangular', 'arrays'), ('arrays', 'access...   \n",
       "2     [('segfault', 'adding'), ('adding', 'variable'...   \n",
       "3     [('make', 'jquery'), ('jquery', 'modify'), ('m...   \n",
       "4     [('xslt', 'buddy'), ('buddy', 'available'), ('...   \n",
       "...                                                 ...   \n",
       "1040  [('magento', 'country'), ('country', 'codes'),...   \n",
       "1041  [('use', 'panel'), ('panel', 'region'), ('regi...   \n",
       "1042  [('work', 'canvas'), ('canvas', 'jcanvas'), ('...   \n",
       "1043  [('soap', 'formatting'), ('formatting', 'probl...   \n",
       "1044  [('use', 'extensibility'), ('extensibility', '...   \n",
       "\n",
       "                                        QuestionsTokens  \\\n",
       "0     ['add', 'bottom', 'padding', 'div', 'contains'...   \n",
       "1     ['rectangular', 'arrays', 'access', 'loop', 'r...   \n",
       "2     ['mime', 'type', 'json', 'returned', 'rest', '...   \n",
       "3     ['segfault', 'adding', 'variable', 'linq', 'sq...   \n",
       "4     ['cocoa', 'wo', 'capture', 'shift', 'modifier'...   \n",
       "...                                                 ...   \n",
       "1040  ['use', 'panel', 'region', 'prism', 'prism', '...   \n",
       "1041  ['magento', 'country', 'codes', 'table', 'rate...   \n",
       "1042  ['use', 'extensibility', 'dll', 'designer', 's...   \n",
       "1043  ['work', 'canvas', 'jcanvas', 'netbeans', 'net...   \n",
       "1044  ['soap', 'formatting', 'problems', 'change', '...   \n",
       "\n",
       "                                          AnswersTokens  \\\n",
       "0     ['box', 'model', 'hack', 'basically', 'providi...   \n",
       "1     ['benchmark', 'results', 'access', 'arr1', 'th...   \n",
       "2     ['common', 'use', 'cases', 'find', 'strings', ...   \n",
       "3     ['used', 'virtualization', 'approach', 'using'...   \n",
       "4     ['years', 'ago', 'iphone', 'web', 'browsers', ...   \n",
       "...                                                 ...   \n",
       "1040  ['answer', 'found', 'nice', 'descriptive', 'bl...   \n",
       "1041  ['appears', 'answers', 'lie', 'following', 'fi...   \n",
       "1042  ['asked', 'different', 'question', 'bit', 'foc...   \n",
       "1043  ['write', 'class', 'extends', 'canvas', 'jcanv...   \n",
       "1044  ['okay', 'ended', 'using', 'soap', 'rpc', 'dri...   \n",
       "\n",
       "                                       QuestionsBigrams  \\\n",
       "0     [('add', 'bottom'), ('bottom', 'padding'), ('p...   \n",
       "1     [('rectangular', 'arrays'), ('arrays', 'access...   \n",
       "2     [('mime', 'type'), ('type', 'json'), ('json', ...   \n",
       "3     [('segfault', 'adding'), ('adding', 'variable'...   \n",
       "4     [('cocoa', 'wo'), ('wo', 'capture'), ('capture...   \n",
       "...                                                 ...   \n",
       "1040  [('use', 'panel'), ('panel', 'region'), ('regi...   \n",
       "1041  [('magento', 'country'), ('country', 'codes'),...   \n",
       "1042  [('use', 'extensibility'), ('extensibility', '...   \n",
       "1043  [('work', 'canvas'), ('canvas', 'jcanvas'), ('...   \n",
       "1044  [('soap', 'formatting'), ('formatting', 'probl...   \n",
       "\n",
       "                                         AnswersBigrams  community_size  \n",
       "0     [('box', 'model'), ('model', 'hack'), ('hack',...             871  \n",
       "1     [('benchmark', 'results'), ('results', 'access...             125  \n",
       "2     [('common', 'use'), ('use', 'cases'), ('cases'...             116  \n",
       "3     [('used', 'virtualization'), ('virtualization'...            1763  \n",
       "4     [('years', 'ago'), ('ago', 'iphone'), ('iphone...             300  \n",
       "...                                                 ...             ...  \n",
       "1040  [('answer', 'found'), ('found', 'nice'), ('nic...               1  \n",
       "1041  [('appears', 'answers'), ('answers', 'lie'), (...               1  \n",
       "1042  [('asked', 'different'), ('different', 'questi...               1  \n",
       "1043  [('write', 'class'), ('class', 'extends'), ('e...               1  \n",
       "1044  [('okay', 'ended'), ('ended', 'using'), ('usin...               1  \n",
       "\n",
       "[1045 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AllTokens</th>\n      <th>AllBigrams</th>\n      <th>QuestionsTokens</th>\n      <th>AnswersTokens</th>\n      <th>QuestionsBigrams</th>\n      <th>AnswersBigrams</th>\n      <th>community_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>['add', 'bottom', 'padding', 'div', 'contains'...</td>\n      <td>[('add', 'bottom'), ('bottom', 'padding'), ('p...</td>\n      <td>['add', 'bottom', 'padding', 'div', 'contains'...</td>\n      <td>['box', 'model', 'hack', 'basically', 'providi...</td>\n      <td>[('add', 'bottom'), ('bottom', 'padding'), ('p...</td>\n      <td>[('box', 'model'), ('model', 'hack'), ('hack',...</td>\n      <td>871</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>['rectangular', 'arrays', 'access', 'loop', 'r...</td>\n      <td>[('rectangular', 'arrays'), ('arrays', 'access...</td>\n      <td>['rectangular', 'arrays', 'access', 'loop', 'r...</td>\n      <td>['benchmark', 'results', 'access', 'arr1', 'th...</td>\n      <td>[('rectangular', 'arrays'), ('arrays', 'access...</td>\n      <td>[('benchmark', 'results'), ('results', 'access...</td>\n      <td>125</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>['segfault', 'adding', 'variable', 'linq', 'sq...</td>\n      <td>[('segfault', 'adding'), ('adding', 'variable'...</td>\n      <td>['mime', 'type', 'json', 'returned', 'rest', '...</td>\n      <td>['common', 'use', 'cases', 'find', 'strings', ...</td>\n      <td>[('mime', 'type'), ('type', 'json'), ('json', ...</td>\n      <td>[('common', 'use'), ('use', 'cases'), ('cases'...</td>\n      <td>116</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>['make', 'jquery', 'modify', 'one', 'div', 'in...</td>\n      <td>[('make', 'jquery'), ('jquery', 'modify'), ('m...</td>\n      <td>['segfault', 'adding', 'variable', 'linq', 'sq...</td>\n      <td>['used', 'virtualization', 'approach', 'using'...</td>\n      <td>[('segfault', 'adding'), ('adding', 'variable'...</td>\n      <td>[('used', 'virtualization'), ('virtualization'...</td>\n      <td>1763</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>['xslt', 'buddy', 'available', 'somewhere', 's...</td>\n      <td>[('xslt', 'buddy'), ('buddy', 'available'), ('...</td>\n      <td>['cocoa', 'wo', 'capture', 'shift', 'modifier'...</td>\n      <td>['years', 'ago', 'iphone', 'web', 'browsers', ...</td>\n      <td>[('cocoa', 'wo'), ('wo', 'capture'), ('capture...</td>\n      <td>[('years', 'ago'), ('ago', 'iphone'), ('iphone...</td>\n      <td>300</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1040</th>\n      <td>['magento', 'country', 'codes', 'table', 'rate...</td>\n      <td>[('magento', 'country'), ('country', 'codes'),...</td>\n      <td>['use', 'panel', 'region', 'prism', 'prism', '...</td>\n      <td>['answer', 'found', 'nice', 'descriptive', 'bl...</td>\n      <td>[('use', 'panel'), ('panel', 'region'), ('regi...</td>\n      <td>[('answer', 'found'), ('found', 'nice'), ('nic...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1041</th>\n      <td>['use', 'panel', 'region', 'prism', 'prism', '...</td>\n      <td>[('use', 'panel'), ('panel', 'region'), ('regi...</td>\n      <td>['magento', 'country', 'codes', 'table', 'rate...</td>\n      <td>['appears', 'answers', 'lie', 'following', 'fi...</td>\n      <td>[('magento', 'country'), ('country', 'codes'),...</td>\n      <td>[('appears', 'answers'), ('answers', 'lie'), (...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1042</th>\n      <td>['work', 'canvas', 'jcanvas', 'netbeans', 'net...</td>\n      <td>[('work', 'canvas'), ('canvas', 'jcanvas'), ('...</td>\n      <td>['use', 'extensibility', 'dll', 'designer', 's...</td>\n      <td>['asked', 'different', 'question', 'bit', 'foc...</td>\n      <td>[('use', 'extensibility'), ('extensibility', '...</td>\n      <td>[('asked', 'different'), ('different', 'questi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1043</th>\n      <td>['soap', 'formatting', 'problems', 'change', '...</td>\n      <td>[('soap', 'formatting'), ('formatting', 'probl...</td>\n      <td>['work', 'canvas', 'jcanvas', 'netbeans', 'net...</td>\n      <td>['write', 'class', 'extends', 'canvas', 'jcanv...</td>\n      <td>[('work', 'canvas'), ('canvas', 'jcanvas'), ('...</td>\n      <td>[('write', 'class'), ('class', 'extends'), ('e...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1044</th>\n      <td>['use', 'extensibility', 'dll', 'designer', 's...</td>\n      <td>[('use', 'extensibility'), ('extensibility', '...</td>\n      <td>['soap', 'formatting', 'problems', 'change', '...</td>\n      <td>['okay', 'ended', 'using', 'soap', 'rpc', 'dri...</td>\n      <td>[('soap', 'formatting'), ('formatting', 'probl...</td>\n      <td>[('okay', 'ended'), ('ended', 'using'), ('usin...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1045 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "community_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('use', 35848), ('using', 28305), ('code', 27964), ('like', 27588), ('would', 26365), ('new', 22463), ('one', 20902), ('get', 20736), ('want', 20493), ('class', 20278)]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "1045",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/work3/s204161/miniconda/lib/python3.10/site-packages/pandas/core/indexes/range.py:391\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 1045 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m flist \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(communities)):\n\u001b[0;32m---> 19\u001b[0m     fdist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(\u001b[43mcommunity_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAllTokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     20\u001b[0m     flist[i] \u001b[38;5;241m=\u001b[39m fdist\n",
      "File \u001b[0;32m/work3/s204161/miniconda/lib/python3.10/site-packages/pandas/core/indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1070\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1072\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work3/s204161/miniconda/lib/python3.10/site-packages/pandas/core/indexing.py:1312\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m-> 1312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work3/s204161/miniconda/lib/python3.10/site-packages/pandas/core/indexing.py:1260\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[0;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work3/s204161/miniconda/lib/python3.10/site-packages/pandas/core/generic.py:4056\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   4054\u001b[0m             new_index \u001b[38;5;241m=\u001b[39m index[loc]\n\u001b[1;32m   4055\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4056\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4058\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m   4059\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m loc\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_:\n",
      "File \u001b[0;32m/work3/s204161/miniconda/lib/python3.10/site-packages/pandas/core/indexes/range.py:393\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 1045"
     ]
    }
   ],
   "source": [
    "#find top 9 communities by number of authors\n",
    "top_9_communities = community_df.sort_values(by=['community_size'], ascending=False).iloc[:9]\n",
    "\n",
    "#not clear if for each of 9 or across all 9 communities, across all is given:\n",
    "#10 most frequent tokens in top 9 communities\n",
    "fv = None\n",
    "for i in range(9):\n",
    "    fdist = nltk.FreqDist(top_9_communities['AllTokens'].iloc[i])\n",
    "    if fv == None:\n",
    "        fv = fdist\n",
    "    else:\n",
    "        fv = fv + fdist\n",
    "fdist = fv\n",
    "print(fdist.most_common(10))\n",
    "\n",
    "#Token frequency distribution for each community:\n",
    "flist = {}\n",
    "for i in range(len(communities)):\n",
    "    fdist = nltk.FreqDist(community_df['AllTokens'].loc[i])\n",
    "    flist[i] = fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            AllTokens  \\\n",
       "1   [windows, xp, firewall, bug, css, issue, link,...   \n",
       "4   [restful, way, monitoring, rest, resource, cha...   \n",
       "..                                                ...   \n",
       "32  [observer, possible, block, response, processi...   \n",
       "2   [add, bottom, padding, div, contains, floating...   \n",
       "\n",
       "                                           AllBigrams  \\\n",
       "1   [(windows, xp), (xp, firewall), (firewall, bug...   \n",
       "4   [(restful, way), (way, monitoring), (monitorin...   \n",
       "..                                                ...   \n",
       "32  [(observer, possible), (possible, block), (blo...   \n",
       "2   [(add, bottom), (bottom, padding), (padding, d...   \n",
       "\n",
       "                                      QuestionsTokens  \\\n",
       "1   [use, namespaces, sql, xml, query, nodes, comm...   \n",
       "4   [initialize, multidimensional, array, use, mem...   \n",
       "..                                                ...   \n",
       "32  [linux, system, manage, configurations, server...   \n",
       "2   [add, bottom, padding, div, contains, floating...   \n",
       "\n",
       "                                        AnswersTokens  \\\n",
       "1   [found, issue, thanks, mark, namespaces, must,...   \n",
       "4   [int, myarray, myarray, new, int, tony, make, ...   \n",
       "..                                                ...   \n",
       "32  [create, separate, program, started, root, pri...   \n",
       "2   [box, model, hack, basically, providing, ie, s...   \n",
       "\n",
       "                                     QuestionsBigrams  \\\n",
       "1   [(use, namespaces), (namespaces, sql), (sql, x...   \n",
       "4   [(initialize, multidimensional), (multidimensi...   \n",
       "..                                                ...   \n",
       "32  [(linux, system), (system, manage), (manage, c...   \n",
       "2   [(add, bottom), (bottom, padding), (padding, d...   \n",
       "\n",
       "                                       AnswersBigrams  community_size  \n",
       "1   [(found, issue), (issue, thanks), (thanks, mar...            5171  \n",
       "4   [(int, myarray), (myarray, myarray), (myarray,...            3433  \n",
       "..                                                ...             ...  \n",
       "32  [(create, separate), (separate, program), (pro...             968  \n",
       "2   [(box, model), (model, hack), (hack, basically...             871  \n",
       "\n",
       "[9 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AllTokens</th>\n      <th>AllBigrams</th>\n      <th>QuestionsTokens</th>\n      <th>AnswersTokens</th>\n      <th>QuestionsBigrams</th>\n      <th>AnswersBigrams</th>\n      <th>community_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>[windows, xp, firewall, bug, css, issue, link,...</td>\n      <td>[(windows, xp), (xp, firewall), (firewall, bug...</td>\n      <td>[use, namespaces, sql, xml, query, nodes, comm...</td>\n      <td>[found, issue, thanks, mark, namespaces, must,...</td>\n      <td>[(use, namespaces), (namespaces, sql), (sql, x...</td>\n      <td>[(found, issue), (issue, thanks), (thanks, mar...</td>\n      <td>5171</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[restful, way, monitoring, rest, resource, cha...</td>\n      <td>[(restful, way), (way, monitoring), (monitorin...</td>\n      <td>[initialize, multidimensional, array, use, mem...</td>\n      <td>[int, myarray, myarray, new, int, tony, make, ...</td>\n      <td>[(initialize, multidimensional), (multidimensi...</td>\n      <td>[(int, myarray), (myarray, myarray), (myarray,...</td>\n      <td>3433</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>[observer, possible, block, response, processi...</td>\n      <td>[(observer, possible), (possible, block), (blo...</td>\n      <td>[linux, system, manage, configurations, server...</td>\n      <td>[create, separate, program, started, root, pri...</td>\n      <td>[(linux, system), (system, manage), (manage, c...</td>\n      <td>[(create, separate), (separate, program), (pro...</td>\n      <td>968</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[add, bottom, padding, div, contains, floating...</td>\n      <td>[(add, bottom), (bottom, padding), (padding, d...</td>\n      <td>[add, bottom, padding, div, contains, floating...</td>\n      <td>[box, model, hack, basically, providing, ie, s...</td>\n      <td>[(add, bottom), (bottom, padding), (padding, d...</td>\n      <td>[(box, model), (model, hack), (hack, basically...</td>\n      <td>871</td>\n    </tr>\n  </tbody>\n</table>\n<p>9 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 395
    }
   ],
   "source": [
    "top_9_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/work3/s204161/miniconda/lib/python3.10/threading.py:247\u001b[0m, in \u001b[0;36mCondition.__init__\u001b[0;34m(self, lock)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_release_save \u001b[38;5;241m=\u001b[39m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_release_save\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_thread.lock' object has no attribute '_release_save'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[508], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m thread_lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m unique_tokens:\n\u001b[0;32m---> 23\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mthreading\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mThread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midf_for_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommunities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommunity_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     thread_list\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# start the threads\u001b[39;00m\n",
      "File \u001b[0;32m/work3/s204161/miniconda/lib/python3.10/threading.py:876\u001b[0m, in \u001b[0;36mThread.__init__\u001b[0;34m(self, group, target, name, args, kwargs, daemon)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_native_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tstate_lock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 876\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_started \u001b[38;5;241m=\u001b[39m \u001b[43mEvent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/work3/s204161/miniconda/lib/python3.10/threading.py:546\u001b[0m, in \u001b[0;36mEvent.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond \u001b[38;5;241m=\u001b[39m \u001b[43mCondition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/work3/s204161/miniconda/lib/python3.10/threading.py:247\u001b[0m, in \u001b[0;36mCondition.__init__\u001b[0;34m(self, lock)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# If the lock defines _release_save() and/or _acquire_restore(),\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# these override the default implementations (which just call\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# release() and acquire() on the lock).  Ditto for _is_owned().\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_release_save \u001b[38;5;241m=\u001b[39m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_release_save\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#idf for each token in corpus.\n",
    "import math\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "def idf_for_token(token, communities,community_df):\n",
    "    \n",
    "    #faster time complexity than code from b4\n",
    "    n_t = sum(community_df['AllTokens'].apply(lambda x:token in x))\n",
    "    \n",
    "    idf = int(math.log(len(communities)/n_t,10))\n",
    "    with thread_lock:\n",
    "        idf_dict[token] = idf\n",
    "    return idf\n",
    "\n",
    "tok_list =community_df['AllTokens']\n",
    "unique_tokens  = list(set(list(itertools.chain.from_iterable(tok_list))))\n",
    "\n",
    "idf_dict, thread_list = {}, [] #thread_list used for multithreading\n",
    "thread_lock = threading.Lock()\n",
    "\n",
    "for token in unique_tokens:\n",
    "    t = threading.Thread(target=idf_for_token, args=(token, communities, community_df))\n",
    "    thread_list.append(t)\n",
    "# start the threads\n",
    "for thread in tqdm(thread_list):\n",
    "    thread.start()\n",
    "# wait for all threads to complete\n",
    "for thread in thread_list:\n",
    "    thread.join()\n",
    "\n",
    "\n",
    "##  IF RAN USING HPC - USE MULTITHREAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "#Running using notebook, on login-node/personal pc\n",
    "tok_list =community_df['AllTokens']\n",
    "unique_tokens  = list(set(list(itertools.chain.from_iterable(tok_list))))\n",
    "\n",
    "idf_dict = {}\n",
    "def idf_for_token(token, communities,community_df):\n",
    "    \n",
    "    #faster time complexity than code from b4\n",
    "    n_t = sum(community_df['AllTokens'].apply(lambda x:token in x))\n",
    "    \n",
    "    idf = int(math.log(len(communities)/n_t,10))\n",
    "    return idf\n",
    "\n",
    "for token in tqdm(unique_tokens):\n",
    "    idf_dict[token] = idf_for_token(token, communities,community_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/work3/s204161/comp_social_science_data/all_tokens_not_hpc.json', 'w') as outfile:\n",
    "    json.dump(idf_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Much Faster Method But Consumes a lot of RAM. Has to be ran on HPC.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# community_df = pd.read_csv('/work3/s204161/comp_social_science_data/community_df.csv', encoding='utf-8')\n",
    "# community_df_a3 = pd.read_csv('/work3/s204161/comp_social_science_data/community_df_alpha3.csv', encoding='utf-8')\n",
    "\n",
    "# community_df.AllTokens = community_df.AllTokens.apply(lambda x: literal_eval(str(x)))\n",
    "# community_df.AllBigrams = community_df.AllBigrams.apply(lambda x: literal_eval(str(x)))\n",
    "\n",
    "# community_df_a3.AllTokens = community_df_a3.AllTokens.apply(lambda x: literal_eval(str(x)))\n",
    "# community_df_a3.AllBigrams = community_df_a3.AllBigrams.apply(lambda x: literal_eval(str(x)))\n",
    "\n",
    "\n",
    "string_comms = [' '.join(doc) for doc in list(community_df['AllTokens'])]\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(2,2),norm=None)\n",
    "\n",
    "bi_fit = vectorizer.fit_transform(string_comms)\n",
    "#tf_idf_scores = bi_fit.toarray()\n",
    "bi_names = (vectorizer.get_feature_names())\n",
    "tf_idf_scores = bi_fit.sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_idf_list = []\n",
    "for column_index,term in enumerate(bi_names):\n",
    "    bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = vectorizer.fit_transform(string_comms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "toke_list = community_df['AllBigrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('implementation', 'special')"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "list(set(list(itertools.chain.from_iterable(toke_list))))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 17.8 GiB for an array with shape (1045, 2291092) and data type float64",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (\u001b[43mX1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/work3/s204161/miniconda/lib/python3.10/site-packages/scipy/sparse/_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1050\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1051\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/work3/s204161/miniconda/lib/python3.10/site-packages/scipy/sparse/_base.py:1291\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 17.8 GiB for an array with shape (1045, 2291092) and data type float64"
     ]
    }
   ],
   "source": [
    "(X1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, tuple found",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m bigram_docs \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mlist\u001b[39m(community_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllBigrams\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# create a Dictionary object from the tokenized documents\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m \u001b[43mDictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbigram_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# convert the tokenized documents into bag-of-words vectors\u001b[39;00m\n\u001b[1;32m     13\u001b[0m corpus \u001b[38;5;241m=\u001b[39m [dictionary\u001b[38;5;241m.\u001b[39mdoc2bow(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m bigram_docs]\n",
      "File \u001b[0;32m/work3/s204161/miniconda/lib/python3.10/site-packages/gensim/corpora/dictionary.py:78\u001b[0m, in \u001b[0;36mDictionary.__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_nnz \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprune_at\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprune_at\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     81\u001b[0m         msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents (total \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m corpus positions)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     82\u001b[0m     )\n",
      "File \u001b[0;32m/work3/s204161/miniconda/lib/python3.10/site-packages/gensim/corpora/dictionary.py:204\u001b[0m, in \u001b[0;36mDictionary.add_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    201\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madding document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, docno, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# update Dictionary with the document\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc2bow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ignore the result, here we only care about updating token ids\u001b[39;00m\n\u001b[1;32m    206\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilt \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m documents (total \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m corpus positions)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_docs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos)\n",
      "File \u001b[0;32m/work3/s204161/miniconda/lib/python3.10/site-packages/gensim/corpora/dictionary.py:246\u001b[0m, in \u001b[0;36mDictionary.doc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    244\u001b[0m counter \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m document:\n\u001b[0;32m--> 246\u001b[0m     counter[w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    248\u001b[0m token2id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_update \u001b[38;5;129;01mor\u001b[39;00m return_missing:\n",
      "\u001b[0;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, tuple found"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# define the set of tokenized documents\n",
    "bigram_docs =  list(community_df['AllBigrams'])\n",
    "\n",
    "# create bigrams from the tokenized documents\n",
    "bigram_phrases = Phrases(docs, min_count=1, threshold=1)\n",
    "bigram_phraser = Phraser(bigram_phrases)\n",
    "bigram_docs = [bigram_phraser[doc] for doc in docs]\n",
    "print(bigram_docs)\n",
    "\n",
    "# create a Dictionary object from the tokenized documents\n",
    "dictionary = Dictionary(bigram_docs)\n",
    "\n",
    "# convert the tokenized documents into bag-of-words vectors\n",
    "corpus = [dictionary.doc2bow(doc) for doc in bigram_docs]\n",
    "\n",
    "# compute the TF-IDF values using the TfidfModel\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "\n",
    "# create a dataframe from the results\n",
    "data = []\n",
    "for doc in tfidf_corpus:\n",
    "    data.extend([(dictionary[id], freq) for id, freq in doc])\n",
    "df = pd.DataFrame(data, columns=['term', 'score'])\n",
    "\n",
    "# get the top 7 ranking bigrams\n",
    "bigrams = [term for term in dictionary.token2id.keys() if '_' in term]\n",
    "words = df[df['term'].isin(bigrams)].sort_values('score', ascending=False).head(7)\n",
    "\n",
    "print('Top 7 ranking bigrams:\\n', words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}