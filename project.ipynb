{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The central idea behind the project is to research communities on a subset of StackOverFlow Q&A using a network describing interactions between users. It is interesting to answer questions like- What is the modularity of the Network, How discernable are the communities from each other in terms of topics discussed & How do they differ? A hypothesis could be that each community uses primarily their own programming language. We could also google the top 5 authors by score or amount of activity to see their specialization - It can also be interesting to look at score, does the highest scoring post depend on community size and to what degree if? Are some communities better at resolving questions? Look at proportion of closed to non-closed questions.\n",
    "\n",
    "The dataset we use for this is 10% of StackOverflow Q&A version 2 from 2019 & contains 2~ million~ questions & 1.2~ million answers. Also write features in both in powerpoint.\n",
    "\n",
    "Dataset is licensed under CC-BY-SA 3.0 with attribution required - meaning it can freely be modified & used both for commercial and research purposes as long as attributed. Top contributors are Miljan Stojiljkovic, Niyamat Ullah, Kartik Garg - See link for rest of contributors.\n",
    "https://www.kaggle.com/datasets/stackoverflow/stacksample\n",
    "\n",
    "\n",
    "Outline:\n",
    "    * Download Dataset. - Done\n",
    "    * Clean out rows with NaN (Except if the NaN is because a question is still open) and clean out stopwords, links, line breaks, etc.- Done\n",
    "    * DataScrape to add UserNames/Names using OwnerUserId, requests & stackexchange API: https://api.stackexchange.com/docs\n",
    "        PS! Since bandwith/quota is not very little compared to dataset size, only top author names are found instead of whole dataset.\n",
    "        \n",
    "    * Make Directed Network with Networkx and calculate best community split using Louvain. Nodes are persons and an edge from one person to another means that the first person replies to a question written by the other person. If multiple replies to same question, count this and add both as attribute.\n",
    "    * WordCloud for best community splits + other diagnostics to analyse social network.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading and Removing NA Rows. Is important to not use for loop, when a solution of constant time complexity exists, when handling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "answers = '/work3/s204161/comp_social_science_data/Answers.csv'\n",
    "questions = '/work3/s204161/comp_social_science_data/Questions.csv'\n",
    "tags = '/work3/s204161/comp_social_science_data/Tags.csv'\n",
    "\n",
    "answers_df = pd.read_csv(answers, encoding='ISO-8859-1')\n",
    "questions_df = pd.read_csv(questions, encoding='ISO-8859-1')\n",
    "tags_df = pd.read_csv(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Answers_df\nSize before: 2014516\nSize After: 2001316\nQuestions_df\nSize before: 1264216\nSize After: 1249762\n"
     ]
    }
   ],
   "source": [
    "#clean rows with NA values in them.\n",
    "clean_answers_df = answers_df.dropna(subset=answers_df.columns, axis=0, how='any')\n",
    "clean_questions_df = questions_df.dropna(subset=questions_df.columns.difference(['ClosedDate']), axis=0, how='any')\n",
    "print(f'Answers_df\\nSize before: {len(answers_df.index)}\\nSize After: {len(clean_answers_df.index)}')\n",
    "print(f'Questions_df\\nSize before: {len(questions_df.index)}\\nSize After: {len(clean_questions_df.index)}')\n",
    "\n",
    "clean_questions_df['OwnerUserId'] = np.asarray(clean_questions_df.get('OwnerUserId'),dtype=int)\n",
    "clean_answers_df['OwnerUserId'] = np.asarray(clean_answers_df.get('OwnerUserId'),dtype=int)\n",
    "\n",
    "clean_answers_df.to_csv('/work3/s204161/comp_social_science_data/no_NA_Answers.csv', encoding='ISO-8859-1', index=False)\n",
    "clean_questions_df.to_csv('/work3/s204161/comp_social_science_data/no_NA_Questions.csv', encoding='ISO-8859-1', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               Id  OwnerUserId          CreationDate  ParentId  Score  \\\n",
       "0              92           61  2008-08-01T14:45:37Z        90     13   \n",
       "1             124           26  2008-08-01T16:09:47Z        80     12   \n",
       "2             199           50  2008-08-01T19:36:46Z       180      1   \n",
       "3             269           91  2008-08-01T23:49:57Z       260      4   \n",
       "4             307           49  2008-08-02T01:49:46Z       260     28   \n",
       "...           ...          ...                   ...       ...    ...   \n",
       "2014511  40143247       333403  2016-10-19T23:42:35Z  40143190      0   \n",
       "2014512  40143322       642706  2016-10-19T23:50:35Z  40137110      1   \n",
       "2014513  40143336      2239781  2016-10-19T23:52:08Z  40141860      0   \n",
       "2014514  40143349      6934347  2016-10-19T23:54:02Z  40077010      0   \n",
       "2014515  40143389      4464432  2016-10-19T23:58:58Z  40142910      0   \n",
       "\n",
       "                                                      Body  \n",
       "0        <p><a href=\"http://svnbook.red-bean.com/\">Vers...  \n",
       "1        <p>I wound up using this. It is a kind of a ha...  \n",
       "2        <p>I've read somewhere the human eye can't dis...  \n",
       "3        <p>Yes, I thought about that, but I soon figur...  \n",
       "4        <p><a href=\"http://www.codeproject.com/Article...  \n",
       "...                                                    ...  \n",
       "2014511  <p>Tanks to <a href=\"http://stackoverflow.com/...  \n",
       "2014512  <h1>tl;dr</h1>\\n\\n<pre><code>ZonedDateTime.par...  \n",
       "2014513  <p>I came up with a very dirty workaround. Bef...  \n",
       "2014514  <p>I solved my own problem defining the follow...  \n",
       "2014515  <p>Try add <code>retrun false</code> in the <c...  \n",
       "\n",
       "[2001316 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>OwnerUserId</th>\n      <th>CreationDate</th>\n      <th>ParentId</th>\n      <th>Score</th>\n      <th>Body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>92</td>\n      <td>61</td>\n      <td>2008-08-01T14:45:37Z</td>\n      <td>90</td>\n      <td>13</td>\n      <td>&lt;p&gt;&lt;a href=\"http://svnbook.red-bean.com/\"&gt;Vers...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>124</td>\n      <td>26</td>\n      <td>2008-08-01T16:09:47Z</td>\n      <td>80</td>\n      <td>12</td>\n      <td>&lt;p&gt;I wound up using this. It is a kind of a ha...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>199</td>\n      <td>50</td>\n      <td>2008-08-01T19:36:46Z</td>\n      <td>180</td>\n      <td>1</td>\n      <td>&lt;p&gt;I've read somewhere the human eye can't dis...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>269</td>\n      <td>91</td>\n      <td>2008-08-01T23:49:57Z</td>\n      <td>260</td>\n      <td>4</td>\n      <td>&lt;p&gt;Yes, I thought about that, but I soon figur...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>307</td>\n      <td>49</td>\n      <td>2008-08-02T01:49:46Z</td>\n      <td>260</td>\n      <td>28</td>\n      <td>&lt;p&gt;&lt;a href=\"http://www.codeproject.com/Article...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2014511</th>\n      <td>40143247</td>\n      <td>333403</td>\n      <td>2016-10-19T23:42:35Z</td>\n      <td>40143190</td>\n      <td>0</td>\n      <td>&lt;p&gt;Tanks to &lt;a href=\"http://stackoverflow.com/...</td>\n    </tr>\n    <tr>\n      <th>2014512</th>\n      <td>40143322</td>\n      <td>642706</td>\n      <td>2016-10-19T23:50:35Z</td>\n      <td>40137110</td>\n      <td>1</td>\n      <td>&lt;h1&gt;tl;dr&lt;/h1&gt;\\n\\n&lt;pre&gt;&lt;code&gt;ZonedDateTime.par...</td>\n    </tr>\n    <tr>\n      <th>2014513</th>\n      <td>40143336</td>\n      <td>2239781</td>\n      <td>2016-10-19T23:52:08Z</td>\n      <td>40141860</td>\n      <td>0</td>\n      <td>&lt;p&gt;I came up with a very dirty workaround. Bef...</td>\n    </tr>\n    <tr>\n      <th>2014514</th>\n      <td>40143349</td>\n      <td>6934347</td>\n      <td>2016-10-19T23:54:02Z</td>\n      <td>40077010</td>\n      <td>0</td>\n      <td>&lt;p&gt;I solved my own problem defining the follow...</td>\n    </tr>\n    <tr>\n      <th>2014515</th>\n      <td>40143389</td>\n      <td>4464432</td>\n      <td>2016-10-19T23:58:58Z</td>\n      <td>40142910</td>\n      <td>0</td>\n      <td>&lt;p&gt;Try add &lt;code&gt;retrun false&lt;/code&gt; in the &lt;c...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2001316 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "clean_answers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "np.sort(clean_answers_df['ParentId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "unique_nodes: 1980582\n"
     ]
    }
   ],
   "source": [
    "unique_users = np.unique(np.asarray(np.append(np.append(clean_questions_df.get('OwnerUserId'),clean_answers_df.get('OwnerUserId')),clean_answers_df.get('ParentId')),dtype=int))\n",
    "print(f'unique_nodes: {len(unique_users)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0              177.0\n",
       "1              164.0\n",
       "2              313.0\n",
       "3          2090091.0\n",
       "4              400.0\n",
       "             ...    \n",
       "2014511          NaN\n",
       "2014512          NaN\n",
       "2014513          NaN\n",
       "2014514          NaN\n",
       "2014515          NaN\n",
       "Length: 2011725, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "np.append(clean_questions_df.get('OwnerUserId'),clean_answers_df.get('OwnerUserId')),clean_answers_df.get('ParentId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([      26,       58,       83, ..., 40141860, 40077010, 40142910])"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "np.append(np.append(clean_questions_df.get('OwnerUserId'),clean_answers_df.get('OwnerUserId')),clean_answers_df.get('ParentId'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "cannot import name 'community_louvain' from 'community' (/work3/s204161/miniconda/lib/python3.10/site-packages/community/__init__.py)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommunity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m community_louvain\n\u001b[1;32m      2\u001b[0m best_partition \u001b[38;5;241m=\u001b[39m community_louvain\u001b[38;5;241m.\u001b[39mbest_partition(nx_G)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'community_louvain' from 'community' (/work3/s204161/miniconda/lib/python3.10/site-packages/community/__init__.py)"
     ]
    }
   ],
   "source": [
    "from community import community_louvain\n",
    "best_partition = community_louvain.best_partition(nx_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(unique_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1000\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m remove_indexes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens)):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#remove bad tokens\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([(tokens[i] \u001b[38;5;241m==\u001b[39m bad_token) \u001b[38;5;28;01mfor\u001b[39;00m bad_token \u001b[38;5;129;01min\u001b[39;00m bad_tokens]):\n\u001b[1;32m     23\u001b[0m         remove_indexes\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m([(bad_token \u001b[38;5;129;01min\u001b[39;00m tokens[i]) \u001b[38;5;28;01mfor\u001b[39;00m bad_token \u001b[38;5;129;01min\u001b[39;00m bad_tokens_2]):\n",
      "Cell \u001b[0;32mIn[45], line 22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m remove_indexes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens)):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#remove bad tokens\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([(tokens[i] \u001b[38;5;241m==\u001b[39m bad_token) \u001b[38;5;28;01mfor\u001b[39;00m bad_token \u001b[38;5;129;01min\u001b[39;00m bad_tokens]):\n\u001b[1;32m     23\u001b[0m         remove_indexes\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m([(bad_token \u001b[38;5;129;01min\u001b[39;00m tokens[i]) \u001b[38;5;28;01mfor\u001b[39;00m bad_token \u001b[38;5;129;01min\u001b[39;00m bad_tokens_2]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "tok_list = []\n",
    "bad_tokens = set(['https:', 'http:', 'www.']) | set(nltk.corpus.stopwords.words('english')) | set(string.punctuation)\n",
    "bad_tokens_2 = set(string.punctuation) | set(['[',']','(',')','{','}','<','>','/','\\\\','|',')',']'])\n",
    "\n",
    "for i in range(len(clean_questions_df.get('OwnerUserId'))):\n",
    "    body_text = clean_questions_df.iloc[i]['Body']\n",
    "    clean_body_text = BeautifulSoup(body_text, 'html.parser').get_text()\n",
    "    #print(clean_body_text) 321\n",
    "    if i % 1000 == 0:\n",
    "       print(i)\n",
    "    tokens = nltk.word_tokenize(clean_body_text)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    remove_indexes = []\n",
    "    for i in range(len(tokens)):\n",
    "        #remove bad tokens\n",
    "        if any([(tokens[i] == bad_token) for bad_token in bad_tokens]):\n",
    "            remove_indexes.append(i)\n",
    "        elif any([(bad_token in tokens[i]) for bad_token in bad_tokens_2]):\n",
    "            remove_indexes.append(i)\n",
    "        elif tokens[i].isnumeric():\n",
    "            remove_indexes.append(i)            \n",
    "    \n",
    "    for i in range(len(remove_indexes) - 1, -1, -1):\n",
    "        remove_index = remove_indexes[i]\n",
    "        del tokens[remove_index]\n",
    "\n",
    "    if len(tokens) == 1:\n",
    "        tokens.remove(tokens[0])\n",
    "\n",
    "        #Maybe do biagrams, if not too computationally demanding, to get some temporal context\n",
    "    tok_list.append(tokens)\n",
    "\n",
    "\n",
    "clean_questions_df['tokens'] = tok_list\n",
    "\n",
    "clean_questions_df.to_csv('/work3/s204161/comp_social_science_data/with_token_Questions.csv', encoding='ISO-8859-1', index=False)\n",
    "\n",
    "#This takes some time to run, so is interrupted and ran overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n",
      "600000\n",
      "650000\n",
      "700000\n",
      "750000\n",
      "800000\n",
      "850000\n",
      "900000\n",
      "950000\n",
      "1000000\n",
      "1050000\n",
      "1100000\n",
      "1150000\n",
      "1200000\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "owner_attrs = {}\n",
    "\n",
    "owner_found_bool = {}\n",
    "#Add question/owner attributes to nodes:\n",
    "for i in range(len(clean_questions_df.get('OwnerUserId'))):\n",
    "    question_id = str(int(clean_questions_df.iloc[i].get('Id')))\n",
    "    owner_id = str(int(clean_questions_df.iloc[i].get('OwnerUserId')))\n",
    "    if not owner_found_bool.get(owner_id):\n",
    "        owner_found_bool[owner_id] = True\n",
    "        owner_attrs[owner_id] = {}\n",
    "        owner_attrs[owner_id]['scores'] = {}\n",
    "    owner_attrs[owner_id][question_id] = BeautifulSoup(clean_questions_df.iloc[i].get('Body'), 'html.parser').get_text()\n",
    "    owner_attrs[owner_id]['scores'][question_id] = (clean_questions_df.iloc[i].get('Score'))    \n",
    "    #Should also save question title in graph - implement here:\n",
    "    \n",
    "\n",
    "    if i % 50000 == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n",
      "600000\n",
      "650000\n",
      "700000\n",
      "750000\n",
      "800000\n",
      "850000\n",
      "900000\n",
      "950000\n",
      "1000000\n",
      "1050000\n",
      "1100000\n",
      "1150000\n",
      "1200000\n",
      "1250000\n",
      "1300000\n",
      "1350000\n",
      "1400000\n",
      "1450000\n",
      "1500000\n",
      "1550000\n",
      "1600000\n",
      "1650000\n",
      "1700000\n",
      "1750000\n",
      "1800000\n",
      "1850000\n",
      "1900000\n",
      "1950000\n",
      "2000000\n"
     ]
    }
   ],
   "source": [
    "answers_attrs = {}\n",
    "answers_found_bool = {}\n",
    "\n",
    "collab_network = {}\n",
    "\n",
    "#Add answer attributes to nodes & remember that edge is directed from OwnerUserId --> ParentId:\n",
    "for i in range(len(clean_answers_df.get('OwnerUserId'))):\n",
    "    answers_id = str(int(clean_answers_df.iloc[i].get('Id')))\n",
    "    owner_id = str(int(clean_answers_df.iloc[i].get('OwnerUserId')))\n",
    "    parent_id = str(int(clean_answers_df.iloc[i].get('ParentId')))\n",
    "    if not answers_found_bool.get(owner_id):\n",
    "        answers_found_bool[owner_id] = True\n",
    "        answers_attrs[owner_id] = {}\n",
    "    if not owner_found_bool.get(owner_id):\n",
    "        owner_found_bool[owner_id] = True\n",
    "        owner_attrs[owner_id] = {}\n",
    "        owner_attrs[owner_id]['scores'] = {}\n",
    "\n",
    "    owner_attrs[owner_id][answers_id] = BeautifulSoup(clean_answers_df.iloc[i].get('Body'), 'html.parser').get_text()\n",
    "    \n",
    "    if collab_network.get((owner_id, parent_id)) == None:\n",
    "        collab_network[(owner_id, parent_id)] = 1\n",
    "    else:\n",
    "        collab_network[(owner_id, parent_id)] += 1\n",
    "\n",
    "    #we want to find scores for each person for answers too.\n",
    "    owner_attrs[owner_id]['scores'][answers_id] = clean_answers_df.iloc[i].get('Score')\n",
    "    if i % 50000 == 0:\n",
    "        print(i)\n",
    "\n",
    "nx.set_node_attributes(G, answers_attrs, 'Questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for owner_id in unique_user_ids:\n",
    "    if owner_attrs.get(owner_id):\n",
    "        owner_attrs[owner_id]['best_score'] = np.max(list(owner_attrs[owner_id]['scores'].values()))\n",
    "        owner_attrs[owner_id]['avg_score'] = np.mean(np.asarray(list(owner_attrs[owner_id]['scores'].values()),dtype=float))\n",
    "        owner_attrs[owner_id]['median_score'] = np.median(list(owner_attrs[owner_id]['scores'].values()))\n",
    "\n",
    "\n",
    "nx.set_node_attributes(G, owner_attrs, 'Questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make edgelist and then edges in network.\n",
    "edgelist = [(x[0][0], x[0][1], x[1]) for x in collab_network.items()]\n",
    "\n",
    "G.add_weighted_edges_from(edgelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab_network.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NodeDataView({}, data='90')"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "G.nodes('90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#Save Graph\n",
    "with open('/work3/s204161/comp_social_science_data/stackoverflow_network.pickle', 'wb') as f:\n",
    "    pickle.dump(G, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#Graph is big, so check Google Drive Link instead for both social network and csv files: \n",
    "# https://drive.google.com/drive/folders/11gTEA4omR2T6JZRMFCww7e2B1DcoqpmO?usp=share_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of nodes in G: 1975473\n",
      "Number of edges in G: 1993272\n",
      "Edges for node 61 in G: [('61', '90'), ('61', '24270'), ('61', '47980'), ('61', '51390'), ('61', '142340'), ('61', '526660'), ('61', '1581560'), ('61', '2520220'), ('61', '6242540'), ('61', '6553950')]\n",
      "Info about Node 61 in G: {'Questions': {'scores': {'8800': 63, '787850': 0, '1581560': 4, '92': 13, '33759': 1, '48055': 7, '51394': 2, '142425': 0, '526668': 3, '1782256': 0, '2520280': 8, '6242610': 8, '6556872': 7}, '8800': \"So I've been poking around with C# a bit lately, and all the Generic Collections have me a little confused. Say I wanted to represent a data structure where the head of a tree was a key value pair, and then there is one optional list of key value pairs below that (but no more levels than these). Would this be suitable?\\npublic class TokenTree\\n{\\n    public TokenTree()\\n    {\\n        /* I must admit to not fully understanding this,\\n         * I got it from msdn. As far as I can tell, IDictionary is an\\n         * interface, and Dictionary is the default implementation of\\n         * that interface, right?\\n         */\\n        SubPairs = new Dictionary<string, string>();\\n    }\\n\\n    public string Key;\\n    public string Value;\\n    public IDictionary<string, string> SubPairs;\\n}\\n\\nIt's only really a simple shunt for passing around data.\\n\", '787850': 'I\\'m looking at PyOpenAL for some sound needs with Python (obviously). Documentation is sparse (consisting of a demo script, which doesn\\'t work unmodified) but as far as I can tell, there are two layers. Direct wrapping of OpenAL calls and a lightweight \\'pythonic\\' wrapper - it is the latter I\\'m concerned with. Specifically, how do you clean up correctly? If we take a small example:\\nimport time\\n\\nimport pyopenal\\n\\npyopenal.init(None)\\n\\nl = pyopenal.Listener(22050)\\n\\nb = pyopenal.WaveBuffer(\"somefile.wav\")\\ns = pyopenal.Source()\\ns.buffer = b\\ns.looping = False\\n\\ns.play()\\n\\nwhile s.get_state() == pyopenal.AL_PLAYING:\\n    time.sleep(1)\\n\\npyopenal.quit()\\n\\nAs it is, a message is printed on to the terminal along the lines of \"one source not deleted, one buffer not deleted\". But I am assuming the we can\\'t use the native OpenAL calls with these objects, so how do I clean up correctly?\\nEDIT:\\nI eventually just ditched pyopenal and wrote a small ctypes wrapper over OpenAL and alure (pyopenal exposes the straight OpenAL functions, but I kept getting SIGFPE). Still curious as to what I was supposed to do here.\\n', '1581560': 'I have a patched gdb 6.8, but I can\\'t get any debugging to work. Given this test file:\\nimport std.stdio;\\n\\nvoid main()\\n{\\n    float f = 3.0;\\n    int i = 1;\\n    writeln(f, \" \", i);\\n    f += cast(float)(i / 10.0);\\n    writeln(f, \" \", i);\\n    i++;\\n    f += cast(float)(i / 10.0);\\n    writeln(f, \" \", i);\\n    i += 2;\\n    f += cast(float)(i / 5.0);\\n    writeln(f, \" \", i);\\n}\\n\\nAnd attempting to debug on the command line:\\nbash-4.0 [d]$ dmd -g test.d  # \\'-gc\\' shows the same behaviour.\\nbash-4.0 [d]$ ~/src/gdb-6.8/gdb/gdb test\\nGNU gdb 6.8\\nCopyright (C) 2008 Free Software Foundation, Inc.\\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\\nThis is free software: you are free to change and redistribute it.\\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\\nand \"show warranty\" for details.\\nThis GDB was configured as \"i686-pc-linux-gnu\"...\\n(gdb) list\\n1 ../sysdeps/i386/elf/start.S: No such file or directory.\\n in ../sysdeps/i386/elf/start.S\\n\\nAnd debugging a project with Eclipse\\nUsing -gc:\\nDwarf Error: Cannot find DIE at 0x134e4 referenced from DIE at 0x12bd4 [in module /home/bernard/projects/drl/drl.i386]\\n(gdb) Dwarf Error: Cannot find DIE at 0x1810 referenced from DIE at 0x1b8 [in module /home/bernard/projects/drl/drl.i386]\\n\\nUsing -g:\\n(gdb) Die: DW_TAG_<unknown> (abbrev = 7, offset = 567)\\n has children: FALSE\\n attributes:\\n  DW_AT_byte_size (DW_FORM_data1) constant: 4\\n  DW_AT_type (DW_FORM_ref4) constant ref: 561 (adjusted)\\n  DW_AT_containing_type (DW_FORM_ref4) constant ref: 539 (adjusted)\\nDwarf Error: Cannot find type of die [in module /home/bernard/projects/drl/drl.i386]\\n\\nI\\'ve seen quite a few posts like this on the Digital Mars newsgroup, but all are seemingly ignored. Can anyone shed some light on the situation?\\nI know of ZeroBUGS, but I really want to get gdb working.\\nUpdate:\\nThanks to luca_, on IRC (freenode, #D), I got the simple case (one file) working:\\n(gdb) list Dmain\\n1 void main()\\n2 {\\n3     float f = 3.0;\\n4     int i = 1;\\n5     f += cast(float)(i / 10.0);\\n6     i++;\\n7     f += cast(float)(i / 10.0);\\n8     i += 2;\\n9     f += cast(float)(i / 5.0);\\n10 }\\n(gdb) break  3\\n\\nUnfortunately, my project made up of multiple files dies with a DWARF error.\\nEDIT:\\nAs of 2.036 (I think), the GDB debugging information produced by DMD is correct, and should work as expected.\\n', '92': 'Version Control with Subversion\\nA very good resource for source control in general. Not really TortoiseSVN specific, though.', '33759': 'I know I find OOP useful pretty much solely on a syntactical sugar basis (encapsulation, operator overloading, typechecking). As to the benefits of OOP... I don\\'t know. I don\\'t think it\\'s worse than procedural stuff. \\nOn the lighter side, my OOP lecturer said that OOP is important because otherwise the \"code would have too many loops\". Yeah. Sometimes it\\'s depressing that I pay $500 per paper. :(\\n', '48055': \"I sure as hell can't. Small errors explode into pages and pages of unreadable junk. Usually early in the morning, before coffee. :(\\nMy only advice is to take a deep breath, start at the top and try and parse the important pieces of information. (I know, easier said than done, right?).  \\n\", '51394': \"I wonder how widespread the JVM actually is? In the case of Flash, IE5 preinstalled it, giving it a large automatic user base. But unless the JVM was included with the OS install, users wouldn't have it. I suppose as a developer you target the largest install base, meaning choosing Flash over Java.\\nThere are Java applets here and there; definitely not widespread though.\\n\", '142425': \"I just have a number. First release is 001. Second release's third beta is 002b3, and so on. This is just for personal stuff mind, I don't actually have anything 'released' at the moment, so this is all theory.\\n\", '526668': 'You need a main function:\\n// The arguments are only needed if passing arguments to your program.\\n// You could just use `int main()`.\\nint main(int argc, char** argv)\\n{\\n    Race race;\\n    race.executeRace();\\n}\\n\\nor so, without seeing your specific error. \\n', '1782256': 'The answer, it seems, is to use GDC, if you can stand going back to D 2.015 (this is for D2, I have no idea how old the D1 stuff is). GDB works great.\\n', '2520280': \"Yes, the function must be declared as extern (C).\\nThe calling convention of functions in C and D are different, so you must tell the compiler to use the C convention with extern (C). I don't know why you don't have to do this in C++.\\nSee here for more information on interfacing with C.\\nIt's also worth noting that you can use the C style for declaring function arguments.\\n\", '6242610': \"If you merely want a pointer, the correct way is to use the 'ptr' property (available for all dynamic arrays, not just strings)\\nstr.ptr\\n\\nHowever, if you are wanting something to use with C, to ensure it is nul-terminated, use toStringz\\nimport std.string;\\ntoStringz(str);\\n\\ntoStringz will not perform a copy if the string is already nul terminated. \\n\", '6556872': \"I asked around on IRC, and as far as we can figure out it was never implemented for D1, so we're guessing it's still unimplemented. Furthermore, there is no mention of the feature in The D Programming Language, so the whole thing is up in the air a bit.\\nIf I were you, I would submit a bug against the documentation. \\n\"}}\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of nodes in G: {G.number_of_nodes()}')\n",
    "print(f'Number of edges in G: {G.number_of_edges()}')\n",
    "print(f'Edges for node 61 in G: {G.edges(str(61))}')\n",
    "\n",
    "print(f'Info about Node 61 in G: {G.nodes[str(61)]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below would find username for all users in dataset. Due to Quotas it is not possible however, so after we find top authors in our best community-split, we can request some of the top author names using stackexchange API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "break for test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Since bandwith/quota is not very little compared to dataset size, only top author names are found instead of whole dataset. This code should be applied on top authors only then.\n",
    "import requests\n",
    "import time\n",
    "remaining_users = []\n",
    "username_dict = {}\n",
    "for user_index in range(0,len(unique_user_ids),100):\n",
    "    unsuccesful_tries = 0\n",
    "    url = f'https://api.stackexchange.com/2.3/users/{\";\".join(map(str, unique_user_ids[user_index:user_index + 100]))}?site=meta.stackoverflow'\n",
    "    #url = f'https://api.stackexchange.com/2.3/users/{61}?site=meta.stackoverflow'\n",
    "    response = requests.get(url)\n",
    "    while response.status_code != 200:\n",
    "        print(f'Failed with status code: {response.status_code}')\n",
    "        time.sleep(5)\n",
    "        if unsuccesful_tries == 3:\n",
    "            remaining_users.append(unique_user_ids[user_index:user_index + 100])\n",
    "            continue\n",
    "\n",
    "    #data = response.json()['items'][0]\n",
    "    #username_dict[str(user)] = data['display_name']\n",
    "    print('break for test')\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "source": [
    "set1 = set(np.asarray(clean_answers_df.get('Id'),dtype=str))\n",
    "set2 = set(np.asarray(clean_questions_df.get('Id'),dtype=str))\n",
    "#Id is unique in whole dataset/on stackoverflow page - is not shared between answers & questions. thats why intersection is empty\n",
    "set1.intersection(set2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "#Save Graph\n",
    "with open('/work3/s204161/comp_social_science_data/stackoverflow_network.pickle', 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "#Graph is big, so check Google Drive Link instead for both social network and csv files: \n",
    "# https://drive.google.com/drive/folders/11gTEA4omR2T6JZRMFCww7e2B1DcoqpmO?usp=share_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seen_nodes = {}\n",
    "\n",
    "for node in G.nodes:\n",
    "    if seen_nodes.get(int(node)):\n",
    "        print('fucker is seen >:-(', node)\n",
    "    seen_nodes[node] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# NEW CODE STARTS HERE using full tokenized answers & questions\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "answers = '/work3/s204161/comp_social_science_data/with_token_Answers.csv'\n",
    "questions = '/work3/s204161/comp_social_science_data/with_token_Questions.csv'\n",
    "questions_df = pd.read_csv(questions, encoding='utf-8')\n",
    "answers_df = pd.read_csv(answers, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df.tokens = answers_df.tokens.apply(lambda x: literal_eval(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset by Time\n",
    "subset_questions_df = questions_df[questions_df.CreationDate.str[:4].astype('int') == 2009]\n",
    "subset_answers_df = answers_df[(2010 > answers_df.CreationDate.str[:4].astype('int')) & (answers_df.CreationDate.str[:4].astype('int') >= 2009)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dict to ensure linear time instead of quadratic\n",
    "subset_question_dict = {}\n",
    "for ques in list(subset_questions_df.Id):\n",
    "    subset_question_dict[ques] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 92887/92887 [01:36<00:00, 959.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "for ans_index in tqdm.tqdm(range(len(subset_answers_df)-1, -1,-1)):\n",
    "    parid = subset_answers_df.iloc[ans_index].ParentId\n",
    "    if not subset_question_dict.get(parid):\n",
    "        subset_answers_df.drop(subset_answers_df.index[ans_index], inplace=True)\n",
    "        #question_dict.get(subset_answers_df.iloc[ans_index].ParentId)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "question_df_length: 31088\n",
      "answers_df_length: 92887\n",
      "answers_df_length_after_removing answers to questions from 2008 or previous years: 82497\n"
     ]
    }
   ],
   "source": [
    "print('question_df_length:',len(subset_questions_df))\n",
    "print('answers_df_length:',len(answers_df[(2010 > answers_df.CreationDate.str[:4].astype('int')) & (answers_df.CreationDate.str[:4].astype('int') >= 2009)]))\n",
    "print('answers_df_length_after_removing answers to questions from 2008 or previous years:', len(subset_answers_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "unique_nodes: 30133\n"
     ]
    }
   ],
   "source": [
    "unique_users = np.unique(np.asarray(np.asarray(np.append(subset_questions_df.get('OwnerUserId'),subset_answers_df.get('OwnerUserId')),dtype=int),dtype=str))\n",
    "print(f'unique_nodes: {len(unique_users)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "def tokenize_col(dat_frame_df,colname):\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    #tok_list = []\n",
    "    bad_tokens = set(['https:', 'http:', 'www.']) | set(nltk.corpus.stopwords.words('english')) | set(string.punctuation)\n",
    "    bad_tokens_2 = set(string.punctuation) | set(['[',']','(',')','{','}','<','>','/','\\\\','|',')',']'])\n",
    "\n",
    "    tok_list = []\n",
    "    for i in range(len(dat_frame_df.get('OwnerUserId'))):\n",
    "        body_text = dat_frame_df.iloc[i][colname]\n",
    "        clean_body_text = BeautifulSoup(body_text, 'html.parser').get_text()\n",
    "        #print(clean_body_text) 321\n",
    "        if i % 5000 == 0:\n",
    "            print(i)\n",
    "        tokens = nltk.word_tokenize(clean_body_text)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        remove_indexes = []\n",
    "        for i in range(len(tokens)):\n",
    "            #remove bad tokens\n",
    "            if any([(tokens[i] == bad_token) for bad_token in bad_tokens]):\n",
    "                remove_indexes.append(i)\n",
    "            elif any([(bad_token in tokens[i]) for bad_token in bad_tokens_2]):\n",
    "                remove_indexes.append(i)\n",
    "            elif tokens[i].isnumeric():\n",
    "                remove_indexes.append(i)            \n",
    "        \n",
    "        for i in range(len(remove_indexes) - 1, -1, -1):\n",
    "            remove_index = remove_indexes[i]\n",
    "            del tokens[remove_index]\n",
    "\n",
    "        if len(tokens) == 1:\n",
    "            tokens.remove(tokens[0])\n",
    "            #Maybe do biagrams, if not too computationally demanding, to get temporal context\n",
    "        tok_list.append(tokens)\n",
    "\n",
    "    return tok_list \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /zhome/a7/0/155527/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /zhome/a7/0/155527/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "subset_questions_df['TitleTokens'] = tokenize_col(subset_questions_df,'Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_questions_df.to_csv('/work3/s204161/comp_social_science_data/subset_questions.csv', encoding='utf-8', index=False)\n",
    "subset_answers_df.to_csv('/work3/s204161/comp_social_science_data/subset_answers.csv', encoding='utf-8', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "quest_attrs = {}\n",
    "\n",
    "quest_found_bool = {}\n",
    "#Add question/owner attributes to nodes:\n",
    "for i in range(len(subset_questions_df.get('OwnerUserId'))):\n",
    "    question_id = str(subset_questions_df.iloc[i].get('Id'))\n",
    "    owner_id = str(int(subset_questions_df.iloc[i].get('OwnerUserId')))\n",
    "    #If first time user is asking, create dict\n",
    "    if not quest_found_bool.get(owner_id):\n",
    "        quest_found_bool[owner_id] = True\n",
    "        quest_attrs[owner_id] = {}\n",
    "        quest_attrs[owner_id]['Scores'] = {}\n",
    "    #owner_attrs[owner_id][question_id] = BeautifulSoup(subset_questions_df.iloc[i].get('Body'), 'html.parser').get_text()\n",
    "    quest_attrs[owner_id]['Scores'][question_id] = (subset_questions_df.iloc[i].get('Score'))    \n",
    "    #Should also save question title in graph - implement here:\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "nx.set_node_attributes(G, quest_attrs, 'Questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "answers_attrs = {}\n",
    "answers_found_bool = {}\n",
    "\n",
    "collab_network = {}\n",
    "\n",
    "#Add answer attributes to nodes & remember that edge is directed from OwnerUserId --> OwnerUserId (parent):\n",
    "for i in range(len(subset_answers_df.get('OwnerUserId'))):\n",
    "    answers_id = str(subset_answers_df.iloc[i].get('Id'))\n",
    "    owner_id = str(int(subset_answers_df.iloc[i].get('OwnerUserId')))\n",
    "    post_parent_id = str(int(subset_answers_df.iloc[i].get('ParentId')))\n",
    "    owner_parent_id = str(int(subset_questions_df[subset_questions_df.Id == int(post_parent_id)].OwnerUserId))\n",
    "    #if first time user is answering create dict\n",
    "    if not answers_found_bool.get(owner_id):\n",
    "        answers_found_bool[owner_id] = True\n",
    "        answers_attrs[owner_id] = {}\n",
    "        answers_attrs[owner_id]['Scores'] = {}\n",
    "    #owner_attrs[owner_id][answers_id] = BeautifulSoup(clean_answers_df.iloc[i].get('Body'), 'html.parser').get_text()\n",
    "\n",
    "    if not collab_network.get((owner_id, owner_parent_id)):\n",
    "        collab_network[(owner_id, owner_parent_id)] = 1\n",
    "    else:\n",
    "        collab_network[(owner_id, owner_parent_id)] += 1\n",
    "\n",
    "    #we want to find scores for each person for answers too.\n",
    "    answers_attrs[owner_id]['Scores'][answers_id] = subset_answers_df.iloc[i].get('Score')\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "\n",
    "nx.set_node_attributes(G, answers_attrs, 'Answers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Should be updated if calculate for best, avg. median...\n",
    "# for owner_id in unique_users:\n",
    "#     if owner_attrs.get(owner_id):\n",
    "#         owner_attrs[owner_id]['BestScore'] = np.max(list(owner_attrs[owner_id]['Scores'].values()))\n",
    "#         owner_attrs[owner_id]['AvgScore'] = np.mean(np.asarray(list(owner_attrs[owner_id]['scores'].values()),dtype=float))\n",
    "#         owner_attrs[owner_id]['MedianScore'] = np.median(list(owner_attrs[owner_id]['scores'].values()))\n",
    "\n",
    "\n",
    "# nx.set_node_attributes(G, owner_attrs, 'Questions')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make edgelist and then edges in network.\n",
    "edgelist = [(x[0][0], x[0][1], x[1]) for x in collab_network.items()]\n",
    "\n",
    "G.add_weighted_edges_from(edgelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/work3/s204161/comp_social_science_data/stackoverflow_subset_network.pickle', 'wb') as f:\n",
    "    pickle.dump(G, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "InEdgeDataView([('31641', '4'), ('43219', '4'), ('40650', '4'), ('4', '4'), ('130812', '4')])"
      ]
     },
     "metadata": {},
     "execution_count": 340
    }
   ],
   "source": [
    "G.in_edges('404430')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Answers': {'Scores': {'701489': 0, '701781': 3}}}"
      ]
     },
     "metadata": {},
     "execution_count": 348
    }
   ],
   "source": [
    "G.nodes['85162'] #some users are only active once a year, so subsetting by \"time\" will not encapsulate all posts made by a person who is active throughout many years. maybe do it by tag instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              Id  OwnerUserId          CreationDate ClosedDate  Score  \\\n",
       "77691    3558350      85162.0  2010-08-24T15:52:41Z        NaN      1   \n",
       "702614  24291060      85162.0  2014-06-18T16:48:51Z        NaN      0   \n",
       "\n",
       "                                                    Title  \\\n",
       "77691   Cross domain script tag working in FF and Chro...   \n",
       "702614  HQL join query to join to a single row of many...   \n",
       "\n",
       "                                                     Body  \\\n",
       "77691   <p>We are providing a snippit of HTML that our...   \n",
       "702614  <p>This HQL query has been driving me up a wal...   \n",
       "\n",
       "                                                   tokens  \n",
       "77691   ['providing', 'snippit', 'html', 'client', 'em...  \n",
       "702614  ['hql', 'query', 'driving', 'wall', 'hope', 's...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>OwnerUserId</th>\n      <th>CreationDate</th>\n      <th>ClosedDate</th>\n      <th>Score</th>\n      <th>Title</th>\n      <th>Body</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>77691</th>\n      <td>3558350</td>\n      <td>85162.0</td>\n      <td>2010-08-24T15:52:41Z</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>Cross domain script tag working in FF and Chro...</td>\n      <td>&lt;p&gt;We are providing a snippit of HTML that our...</td>\n      <td>['providing', 'snippit', 'html', 'client', 'em...</td>\n    </tr>\n    <tr>\n      <th>702614</th>\n      <td>24291060</td>\n      <td>85162.0</td>\n      <td>2014-06-18T16:48:51Z</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>HQL join query to join to a single row of many...</td>\n      <td>&lt;p&gt;This HQL query has been driving me up a wal...</td>\n      <td>['hql', 'query', 'driving', 'wall', 'hope', 's...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 356
    }
   ],
   "source": [
    "#I\n",
    "questions_df[questions_df.OwnerUserId.astype(int).astype(str) == '85162']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           Id  OwnerUserId          CreationDate  ParentId  Score  \\\n",
       "38133  701489      85162.0  2009-03-31T15:10:50Z    701200      0   \n",
       "38156  701781      85162.0  2009-03-31T16:08:37Z    701200      3   \n",
       "\n",
       "                                                    Body  \\\n",
       "38133  <p>Try to get the object back from your extern...   \n",
       "38156  <p><a href=\"http://osmanu.com/flex/EITest/EITe...   \n",
       "\n",
       "                                                  tokens  \n",
       "38133  [try, get, object, back, external, interface, ...  \n",
       "38156  [working, sample, based, code, provided, right...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>OwnerUserId</th>\n      <th>CreationDate</th>\n      <th>ParentId</th>\n      <th>Score</th>\n      <th>Body</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>38133</th>\n      <td>701489</td>\n      <td>85162.0</td>\n      <td>2009-03-31T15:10:50Z</td>\n      <td>701200</td>\n      <td>0</td>\n      <td>&lt;p&gt;Try to get the object back from your extern...</td>\n      <td>[try, get, object, back, external, interface, ...</td>\n    </tr>\n    <tr>\n      <th>38156</th>\n      <td>701781</td>\n      <td>85162.0</td>\n      <td>2009-03-31T16:08:37Z</td>\n      <td>701200</td>\n      <td>3</td>\n      <td>&lt;p&gt;&lt;a href=\"http://osmanu.com/flex/EITest/EITe...</td>\n      <td>[working, sample, based, code, provided, right...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 357
    }
   ],
   "source": [
    "answers_df[answers_df.OwnerUserId.astype(int).astype(str) == '85162']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0               61.0\n",
       "1               26.0\n",
       "2               50.0\n",
       "3               91.0\n",
       "4               49.0\n",
       "             ...    \n",
       "2001311     333403.0\n",
       "2001312     642706.0\n",
       "2001313    2239781.0\n",
       "2001314    6934347.0\n",
       "2001315    4464432.0\n",
       "Name: OwnerUserId, Length: 2001316, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 355
    }
   ],
   "source": [
    "answers_df.OwnerUserId"
   ]
  }
 ]
}