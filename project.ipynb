{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The central idea behind the project is to research communities on a subset of StackOverFlow Q&A using a network describing interactions between users. It is interesting to answer questions like- What is the modularity of the Network, How discernable are the communities from each other in terms of topics discussed & How do they differ? A hypothesis could be that each community uses primarily their own programming language. We could also google the top 5 authors by score or amount of activity to see their specialization - It can also be interesting to look at score, does the highest scoring post depend on community size and to what degree if? Are some communities better at resolving questions? Look at proportion of closed to non-closed questions.\n",
    "\n",
    "The dataset we use for this is 10% of StackOverflow Q&A version 2 from 2019 & contains 2~ million~ questions & 1.2~ million answers. Also write features in both in powerpoint.\n",
    "\n",
    "Dataset is licensed under CC-BY-SA 3.0 with attribution required - meaning it can freely be modified & used both for commercial and research purposes as long as attributed. Top contributors are Miljan Stojiljkovic, Niyamat Ullah, Kartik Garg - See link for rest of contributors.\n",
    "https://www.kaggle.com/datasets/stackoverflow/stacksample\n",
    "\n",
    "\n",
    "Outline:\n",
    "    * Download Dataset. - Done\n",
    "    * Clean out rows with NaN (Except if the NaN is because a question is still open) and clean out stopwords, links, line breaks, etc.- Done\n",
    "    * DataScrape to add UserNames/Names using OwnerUserId, requests & stackexchange API: https://api.stackexchange.com/docs\n",
    "        PS! Since bandwith/quota is not very little compared to dataset size, only top author names are found instead of whole dataset.\n",
    "        \n",
    "    * Make Directed Network with Networkx and calculate best community split using Louvain. Nodes are persons and an edge from one person to another means that the first person replies to a question written by the other person. If multiple replies to same question, count this and add both as attribute.\n",
    "    * WordCloud for best community splits + other diagnostics to analyse social network.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading and Removing NA Rows. Is important to not use for loop, when a solution of constant time complexity exists, when handling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "answers = '/work3/s204161/comp_social_science_data/Answers.csv'\n",
    "questions = '/work3/s204161/comp_social_science_data/Questions.csv'\n",
    "tags = '/work3/s204161/comp_social_science_data/Tags.csv'\n",
    "\n",
    "answers_df = pd.read_csv(answers, encoding='ISO-8859-1')\n",
    "questions_df = pd.read_csv(questions, encoding='ISO-8859-1')\n",
    "tags_df = pd.read_csv(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Answers_df\nSize before: 2014516\nSize After: 2001316\nQuestions_df\nSize before: 1264216\nSize After: 1249762\n"
     ]
    }
   ],
   "source": [
    "#clean rows with NA values in them.\n",
    "clean_answers_df = answers_df.dropna(subset=answers_df.columns, axis=0, how='any')\n",
    "clean_questions_df = questions_df.dropna(subset=questions_df.columns.difference(['ClosedDate']), axis=0, how='any')\n",
    "print(f'Answers_df\\nSize before: {len(answers_df.index)}\\nSize After: {len(clean_answers_df.index)}')\n",
    "print(f'Questions_df\\nSize before: {len(questions_df.index)}\\nSize After: {len(clean_questions_df.index)}')\n",
    "\n",
    "clean_answers_df.to_csv('/work3/s204161/comp_social_science_data/no_NA_Answers.csv', encoding='ISO-8859-1', index=False)\n",
    "clean_questions_df.to_csv('/work3/s204161/comp_social_science_data/no_NA_Questions.csv', encoding='ISO-8859-1', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Id                                                             92\nOwnerUserId                                                  61.0\nCreationDate                                 2008-08-01T14:45:37Z\nParentId                                                       90\nScore                                                          13\nBody            <p><a href=\"http://svnbook.red-bean.com/\">Vers...\nName: 0, dtype: object\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'<p><a href=\"http://svnbook.red-bean.com/\">Version Control with Subversion</a></p>\\r\\n\\r\\n<p>A very good resource for source control in general. Not really TortoiseSVN specific, though.</p>'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "print(clean_answers_df.iloc[0])\n",
    "clean_answers_df.iloc[0].get('Body')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               Id  OwnerUserId          CreationDate            ClosedDate  \\\n",
       "0              80         26.0  2008-08-01T13:57:07Z                   NaN   \n",
       "1              90         58.0  2008-08-01T14:41:24Z  2012-12-26T03:45:49Z   \n",
       "2             120         83.0  2008-08-01T15:50:08Z                   NaN   \n",
       "3             180    2089740.0  2008-08-01T18:42:19Z                   NaN   \n",
       "4             260         91.0  2008-08-01T23:22:08Z                   NaN   \n",
       "...           ...          ...                   ...                   ...   \n",
       "1264211  40143210    5610777.0  2016-10-19T23:38:01Z                   NaN   \n",
       "1264212  40143300    3791161.0  2016-10-19T23:48:09Z                   NaN   \n",
       "1264213  40143340    7028647.0  2016-10-19T23:52:50Z                   NaN   \n",
       "1264214  40143360     871677.0  2016-10-19T23:55:24Z                   NaN   \n",
       "1264215  40143380    6823982.0  2016-10-19T23:57:31Z                   NaN   \n",
       "\n",
       "         Score                                              Title  \\\n",
       "0           26  SQLStatement.execute() - multiple queries in o...   \n",
       "1          144  Good branching and merging tutorials for Torto...   \n",
       "2           21                                  ASP.NET Site Maps   \n",
       "3           53                 Function for creating color wheels   \n",
       "4           49  Adding scripting functionality to .NET applica...   \n",
       "...        ...                                                ...   \n",
       "1264211      0                           URL routing in PHP (MVC)   \n",
       "1264212      0           Bigquery.Jobs.Insert - Resumable Upload?   \n",
       "1264213      1                 Obfuscating code in android studio   \n",
       "1264214      0         How to fire function after v-model change?   \n",
       "1264215      0            npm run mocha test - files being cached   \n",
       "\n",
       "                                                      Body  \n",
       "0        <p>I've written a database generation script i...  \n",
       "1        <p>Are there any really good tutorials explain...  \n",
       "2        <p>Has anyone got experience creating <strong>...  \n",
       "3        <p>This is something I've pseudo-solved many t...  \n",
       "4        <p>I have a little game written in C#. It uses...  \n",
       "...                                                    ...  \n",
       "1264211  <p>I am building a custom MVC project and I ha...  \n",
       "1264212  <p>The API docs show that you should be able t...  \n",
       "1264213  <p>Under minifyEnabled I changed from false to...  \n",
       "1264214  <p>I have input which I use to filter my array...  \n",
       "1264215  <p>I'm running a mocha test and I noticed my c...  \n",
       "\n",
       "[1249762 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>OwnerUserId</th>\n      <th>CreationDate</th>\n      <th>ClosedDate</th>\n      <th>Score</th>\n      <th>Title</th>\n      <th>Body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>80</td>\n      <td>26.0</td>\n      <td>2008-08-01T13:57:07Z</td>\n      <td>NaN</td>\n      <td>26</td>\n      <td>SQLStatement.execute() - multiple queries in o...</td>\n      <td>&lt;p&gt;I've written a database generation script i...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>90</td>\n      <td>58.0</td>\n      <td>2008-08-01T14:41:24Z</td>\n      <td>2012-12-26T03:45:49Z</td>\n      <td>144</td>\n      <td>Good branching and merging tutorials for Torto...</td>\n      <td>&lt;p&gt;Are there any really good tutorials explain...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>120</td>\n      <td>83.0</td>\n      <td>2008-08-01T15:50:08Z</td>\n      <td>NaN</td>\n      <td>21</td>\n      <td>ASP.NET Site Maps</td>\n      <td>&lt;p&gt;Has anyone got experience creating &lt;strong&gt;...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>180</td>\n      <td>2089740.0</td>\n      <td>2008-08-01T18:42:19Z</td>\n      <td>NaN</td>\n      <td>53</td>\n      <td>Function for creating color wheels</td>\n      <td>&lt;p&gt;This is something I've pseudo-solved many t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>260</td>\n      <td>91.0</td>\n      <td>2008-08-01T23:22:08Z</td>\n      <td>NaN</td>\n      <td>49</td>\n      <td>Adding scripting functionality to .NET applica...</td>\n      <td>&lt;p&gt;I have a little game written in C#. It uses...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1264211</th>\n      <td>40143210</td>\n      <td>5610777.0</td>\n      <td>2016-10-19T23:38:01Z</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>URL routing in PHP (MVC)</td>\n      <td>&lt;p&gt;I am building a custom MVC project and I ha...</td>\n    </tr>\n    <tr>\n      <th>1264212</th>\n      <td>40143300</td>\n      <td>3791161.0</td>\n      <td>2016-10-19T23:48:09Z</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>Bigquery.Jobs.Insert - Resumable Upload?</td>\n      <td>&lt;p&gt;The API docs show that you should be able t...</td>\n    </tr>\n    <tr>\n      <th>1264213</th>\n      <td>40143340</td>\n      <td>7028647.0</td>\n      <td>2016-10-19T23:52:50Z</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>Obfuscating code in android studio</td>\n      <td>&lt;p&gt;Under minifyEnabled I changed from false to...</td>\n    </tr>\n    <tr>\n      <th>1264214</th>\n      <td>40143360</td>\n      <td>871677.0</td>\n      <td>2016-10-19T23:55:24Z</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>How to fire function after v-model change?</td>\n      <td>&lt;p&gt;I have input which I use to filter my array...</td>\n    </tr>\n    <tr>\n      <th>1264215</th>\n      <td>40143380</td>\n      <td>6823982.0</td>\n      <td>2016-10-19T23:57:31Z</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>npm run mocha test - files being cached</td>\n      <td>&lt;p&gt;I'm running a mocha test and I noticed my c...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1249762 rows Ã— 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "clean_questions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "unique_nodes: 1131290\n"
     ]
    }
   ],
   "source": [
    "unique_users = np.unique([x for x in np.unique(clean_questions_df.get('OwnerUserId')+clean_answers_df.get('OwnerUserId'))])\n",
    "print(f'unique_nodes: {len(unique_users)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "unique_user_ids = np.asarray(np.asarray(np.unique(clean_answers_df.get('OwnerUserId')+clean_answers_df.get('OwnerUserId')),dtype=int),dtype=str)\n",
    "pd.isna(np.asarray(np.unique(clean_answers_df.get('OwnerUserId')+clean_answers_df.get('OwnerUserId')),dtype=int)).any() #check if any NAN - should be false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(unique_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /zhome/a7/0/155527/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1000\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m remove_indexes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens)):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#remove bad tokens\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([(tokens[i] \u001b[38;5;241m==\u001b[39m bad_token) \u001b[38;5;28;01mfor\u001b[39;00m bad_token \u001b[38;5;129;01min\u001b[39;00m bad_tokens]):\n\u001b[1;32m     23\u001b[0m         remove_indexes\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m([(bad_token \u001b[38;5;129;01min\u001b[39;00m tokens[i]) \u001b[38;5;28;01mfor\u001b[39;00m bad_token \u001b[38;5;129;01min\u001b[39;00m bad_tokens_2]):\n",
      "Cell \u001b[0;32mIn[45], line 22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m remove_indexes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens)):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#remove bad tokens\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([(tokens[i] \u001b[38;5;241m==\u001b[39m bad_token) \u001b[38;5;28;01mfor\u001b[39;00m bad_token \u001b[38;5;129;01min\u001b[39;00m bad_tokens]):\n\u001b[1;32m     23\u001b[0m         remove_indexes\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m([(bad_token \u001b[38;5;129;01min\u001b[39;00m tokens[i]) \u001b[38;5;28;01mfor\u001b[39;00m bad_token \u001b[38;5;129;01min\u001b[39;00m bad_tokens_2]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "tok_list = []\n",
    "bad_tokens = set(['https:', 'http:', 'www.']) | set(nltk.corpus.stopwords.words('english')) | set(string.punctuation)\n",
    "bad_tokens_2 = set(string.punctuation) | set(['[',']','(',')','{','}','<','>','/','\\\\','|',')',']'])\n",
    "\n",
    "for i in range(len(clean_questions_df.get('OwnerUserId'))):\n",
    "    body_text = clean_questions_df.iloc[i]['Body']\n",
    "    clean_body_text = BeautifulSoup(body_text, 'html.parser').get_text()\n",
    "    #print(clean_body_text) 321\n",
    "    if i % 1000 == 0:\n",
    "       print(i)\n",
    "    tokens = nltk.word_tokenize(clean_body_text)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    remove_indexes = []\n",
    "    for i in range(len(tokens)):\n",
    "        #remove bad tokens\n",
    "        if any([(tokens[i] == bad_token) for bad_token in bad_tokens]):\n",
    "            remove_indexes.append(i)\n",
    "        elif any([(bad_token in tokens[i]) for bad_token in bad_tokens_2]):\n",
    "            remove_indexes.append(i)\n",
    "        elif tokens[i].isnumeric():\n",
    "            remove_indexes.append(i)            \n",
    "    \n",
    "    for i in range(len(remove_indexes) - 1, -1, -1):\n",
    "        remove_index = remove_indexes[i]\n",
    "        del tokens[remove_index]\n",
    "\n",
    "    if len(tokens) == 1:\n",
    "        tokens.remove(tokens[0])\n",
    "\n",
    "        #Maybe do biagrams, if not too computationally demanding, to get some temporal context\n",
    "    tok_list.append(tokens)\n",
    "\n",
    "\n",
    "clean_questions_df['tokens'] = tok_list\n",
    "\n",
    "clean_questions_df.to_csv('/work3/s204161/comp_social_science_data/with_token_Questions.csv', encoding='ISO-8859-1', index=False)\n",
    "\n",
    "#This takes some time to run, so is interrupted and ran overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n",
      "600000\n",
      "650000\n",
      "700000\n",
      "750000\n",
      "800000\n",
      "850000\n",
      "900000\n",
      "950000\n",
      "1000000\n",
      "1050000\n",
      "1100000\n",
      "1150000\n",
      "1200000\n"
     ]
    }
   ],
   "source": [
    "owner_attrs = {}\n",
    "\n",
    "owner_found_bool = {}\n",
    "#Add question/owner attributes to nodes:\n",
    "for i in range(len(clean_questions_df.get('OwnerUserId'))):\n",
    "    question_id = str(clean_questions_df.iloc[i].get('Id'))\n",
    "    owner_id = str(int(clean_questions_df.iloc[i].get('OwnerUserId')))\n",
    "    if not owner_found_bool.get(owner_id):\n",
    "        owner_found_bool[owner_id] = True\n",
    "        owner_attrs[owner_id] = {}\n",
    "        owner_attrs[owner_id]['scores'] = {}\n",
    "    owner_attrs[owner_id][question_id] = BeautifulSoup(clean_questions_df.iloc[i].get('Body'), 'html.parser').get_text()\n",
    "    owner_attrs[owner_id]['scores'][question_id] = (clean_questions_df.iloc[i].get('Score'))    \n",
    "    #Should also save question title in graph - implement here:\n",
    "    \n",
    "\n",
    "    if i % 50000 == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n",
      "600000\n",
      "650000\n",
      "700000\n",
      "750000\n",
      "800000\n",
      "850000\n",
      "900000\n",
      "950000\n",
      "1000000\n",
      "1050000\n",
      "1100000\n",
      "1150000\n",
      "1200000\n",
      "1250000\n",
      "1300000\n",
      "1350000\n",
      "1400000\n",
      "1450000\n",
      "1500000\n",
      "1550000\n",
      "1600000\n",
      "1650000\n",
      "1700000\n",
      "1750000\n",
      "1800000\n",
      "1850000\n",
      "1900000\n",
      "1950000\n",
      "2000000\n"
     ]
    }
   ],
   "source": [
    "answers_attrs = {}\n",
    "answers_found_bool = {}\n",
    "\n",
    "collab_network = {}\n",
    "\n",
    "#Add answer attributes to nodes & remember that edge is directed from OwnerUserId --> ParentId:\n",
    "for i in range(len(clean_answers_df.get('OwnerUserId'))):\n",
    "    answers_id = str(clean_answers_df.iloc[i].get('Id'))\n",
    "    owner_id = str(int(clean_answers_df.iloc[i].get('OwnerUserId')))\n",
    "    parent_id = str(int(clean_answers_df.iloc[i].get('ParentId')))\n",
    "    if not answers_found_bool.get(owner_id):\n",
    "        answers_found_bool[owner_id] = True\n",
    "        answers_attrs[owner_id] = {}\n",
    "    if not owner_found_bool.get(owner_id):\n",
    "        owner_found_bool[owner_id] = True\n",
    "        owner_attrs[owner_id] = {}\n",
    "        owner_attrs[owner_id]['scores'] = {}\n",
    "\n",
    "    owner_attrs[owner_id][answers_id] = BeautifulSoup(clean_answers_df.iloc[i].get('Body'), 'html.parser').get_text()\n",
    "    \n",
    "    if not collab_network.get((owner_id, parent_id)):\n",
    "        collab_network[(owner_id, parent_id)] = 1\n",
    "    else:\n",
    "        collab_network[(owner_id, parent_id)] += 1\n",
    "\n",
    "    #we want to find scores for each person for answers too.\n",
    "    owner_attrs[owner_id]['scores'][answers_id] = clean_answers_df.iloc[i].get('Score')\n",
    "    if i % 50000 == 0:\n",
    "        print(i)\n",
    "\n",
    "nx.set_node_attributes(G, answers_attrs, 'Questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for owner_id in unique_user_ids:\n",
    "    if owner_attrs.get(owner_id):\n",
    "        owner_attrs[owner_id]['best_score'] = np.max(list(owner_attrs[owner_id]['scores'].values()))\n",
    "        owner_attrs[owner_id]['avg_score'] = np.mean(np.asarray(list(owner_attrs[owner_id]['scores'].values()),dtype=float))\n",
    "        owner_attrs[owner_id]['median_score'] = np.median(list(owner_attrs[owner_id]['scores'].values()))\n",
    "\n",
    "\n",
    "nx.set_node_attributes(G, owner_attrs, 'Questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make edgelist and then edges in network.\n",
    "edgelist = [(x[0][0], x[0][1], x[1]) for x in collab_network.items()]\n",
    "\n",
    "G.add_weighted_edges_from(edgelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#Save Graph\n",
    "with open('/work3/s204161/comp_social_science_data/stackoverflow_network.pickle', 'wb') as f:\n",
    "    pickle.dump(G, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#Graph is big, so check Google Drive Link instead for both social network and csv files: \n",
    "# https://drive.google.com/drive/folders/11gTEA4omR2T6JZRMFCww7e2B1DcoqpmO?usp=share_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of nodes in G: 1975473\n",
      "Number of edges in G: 1993272\n",
      "Edges for node 61 in G: [('61', '90'), ('61', '24270'), ('61', '47980'), ('61', '51390'), ('61', '142340'), ('61', '526660'), ('61', '1581560'), ('61', '2520220'), ('61', '6242540'), ('61', '6553950')]\n",
      "Info about Node 61 in G: {'Questions': {'scores': {'8800': 63, '787850': 0, '1581560': 4, '92': 13, '33759': 1, '48055': 7, '51394': 2, '142425': 0, '526668': 3, '1782256': 0, '2520280': 8, '6242610': 8, '6556872': 7}, '8800': \"So I've been poking around with C# a bit lately, and all the Generic Collections have me a little confused. Say I wanted to represent a data structure where the head of a tree was a key value pair, and then there is one optional list of key value pairs below that (but no more levels than these). Would this be suitable?\\npublic class TokenTree\\n{\\n    public TokenTree()\\n    {\\n        /* I must admit to not fully understanding this,\\n         * I got it from msdn. As far as I can tell, IDictionary is an\\n         * interface, and Dictionary is the default implementation of\\n         * that interface, right?\\n         */\\n        SubPairs = new Dictionary<string, string>();\\n    }\\n\\n    public string Key;\\n    public string Value;\\n    public IDictionary<string, string> SubPairs;\\n}\\n\\nIt's only really a simple shunt for passing around data.\\n\", '787850': 'I\\'m looking at PyOpenAL for some sound needs with Python (obviously). Documentation is sparse (consisting of a demo script, which doesn\\'t work unmodified) but as far as I can tell, there are two layers. Direct wrapping of OpenAL calls and a lightweight \\'pythonic\\' wrapper - it is the latter I\\'m concerned with. Specifically, how do you clean up correctly? If we take a small example:\\nimport time\\n\\nimport pyopenal\\n\\npyopenal.init(None)\\n\\nl = pyopenal.Listener(22050)\\n\\nb = pyopenal.WaveBuffer(\"somefile.wav\")\\ns = pyopenal.Source()\\ns.buffer = b\\ns.looping = False\\n\\ns.play()\\n\\nwhile s.get_state() == pyopenal.AL_PLAYING:\\n    time.sleep(1)\\n\\npyopenal.quit()\\n\\nAs it is, a message is printed on to the terminal along the lines of \"one source not deleted, one buffer not deleted\". But I am assuming the we can\\'t use the native OpenAL calls with these objects, so how do I clean up correctly?\\nEDIT:\\nI eventually just ditched pyopenal and wrote a small ctypes wrapper over OpenAL and alure (pyopenal exposes the straight OpenAL functions, but I kept getting SIGFPE). Still curious as to what I was supposed to do here.\\n', '1581560': 'I have a patched gdb 6.8, but I can\\'t get any debugging to work. Given this test file:\\nimport std.stdio;\\n\\nvoid main()\\n{\\n    float f = 3.0;\\n    int i = 1;\\n    writeln(f, \" \", i);\\n    f += cast(float)(i / 10.0);\\n    writeln(f, \" \", i);\\n    i++;\\n    f += cast(float)(i / 10.0);\\n    writeln(f, \" \", i);\\n    i += 2;\\n    f += cast(float)(i / 5.0);\\n    writeln(f, \" \", i);\\n}\\n\\nAnd attempting to debug on the command line:\\nbash-4.0 [d]$ dmd -g test.d  # \\'-gc\\' shows the same behaviour.\\nbash-4.0 [d]$ ~/src/gdb-6.8/gdb/gdb test\\nGNU gdb 6.8\\nCopyright (C) 2008 Free Software Foundation, Inc.\\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\\nThis is free software: you are free to change and redistribute it.\\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\\nand \"show warranty\" for details.\\nThis GDB was configured as \"i686-pc-linux-gnu\"...\\n(gdb) list\\n1 ../sysdeps/i386/elf/start.S: No such file or directory.\\n in ../sysdeps/i386/elf/start.S\\n\\nAnd debugging a project with Eclipse\\nUsing -gc:\\nDwarf Error: Cannot find DIE at 0x134e4 referenced from DIE at 0x12bd4 [in module /home/bernard/projects/drl/drl.i386]\\n(gdb) Dwarf Error: Cannot find DIE at 0x1810 referenced from DIE at 0x1b8 [in module /home/bernard/projects/drl/drl.i386]\\n\\nUsing -g:\\n(gdb) Die: DW_TAG_<unknown> (abbrev = 7, offset = 567)\\n has children: FALSE\\n attributes:\\n  DW_AT_byte_size (DW_FORM_data1) constant: 4\\n  DW_AT_type (DW_FORM_ref4) constant ref: 561 (adjusted)\\n  DW_AT_containing_type (DW_FORM_ref4) constant ref: 539 (adjusted)\\nDwarf Error: Cannot find type of die [in module /home/bernard/projects/drl/drl.i386]\\n\\nI\\'ve seen quite a few posts like this on the Digital Mars newsgroup, but all are seemingly ignored. Can anyone shed some light on the situation?\\nI know of ZeroBUGS, but I really want to get gdb working.\\nUpdate:\\nThanks to luca_, on IRC (freenode, #D), I got the simple case (one file) working:\\n(gdb) list Dmain\\n1 void main()\\n2 {\\n3     float f = 3.0;\\n4     int i = 1;\\n5     f += cast(float)(i / 10.0);\\n6     i++;\\n7     f += cast(float)(i / 10.0);\\n8     i += 2;\\n9     f += cast(float)(i / 5.0);\\n10 }\\n(gdb) break  3\\n\\nUnfortunately, my project made up of multiple files dies with a DWARF error.\\nEDIT:\\nAs of 2.036 (I think), the GDB debugging information produced by DMD is correct, and should work as expected.\\n', '92': 'Version Control with Subversion\\nA very good resource for source control in general. Not really TortoiseSVN specific, though.', '33759': 'I know I find OOP useful pretty much solely on a syntactical sugar basis (encapsulation, operator overloading, typechecking). As to the benefits of OOP... I don\\'t know. I don\\'t think it\\'s worse than procedural stuff. \\nOn the lighter side, my OOP lecturer said that OOP is important because otherwise the \"code would have too many loops\". Yeah. Sometimes it\\'s depressing that I pay $500 per paper. :(\\n', '48055': \"I sure as hell can't. Small errors explode into pages and pages of unreadable junk. Usually early in the morning, before coffee. :(\\nMy only advice is to take a deep breath, start at the top and try and parse the important pieces of information. (I know, easier said than done, right?).  \\n\", '51394': \"I wonder how widespread the JVM actually is? In the case of Flash, IE5 preinstalled it, giving it a large automatic user base. But unless the JVM was included with the OS install, users wouldn't have it. I suppose as a developer you target the largest install base, meaning choosing Flash over Java.\\nThere are Java applets here and there; definitely not widespread though.\\n\", '142425': \"I just have a number. First release is 001. Second release's third beta is 002b3, and so on. This is just for personal stuff mind, I don't actually have anything 'released' at the moment, so this is all theory.\\n\", '526668': 'You need a main function:\\n// The arguments are only needed if passing arguments to your program.\\n// You could just use `int main()`.\\nint main(int argc, char** argv)\\n{\\n    Race race;\\n    race.executeRace();\\n}\\n\\nor so, without seeing your specific error. \\n', '1782256': 'The answer, it seems, is to use GDC, if you can stand going back to D 2.015 (this is for D2, I have no idea how old the D1 stuff is). GDB works great.\\n', '2520280': \"Yes, the function must be declared as extern (C).\\nThe calling convention of functions in C and D are different, so you must tell the compiler to use the C convention with extern (C). I don't know why you don't have to do this in C++.\\nSee here for more information on interfacing with C.\\nIt's also worth noting that you can use the C style for declaring function arguments.\\n\", '6242610': \"If you merely want a pointer, the correct way is to use the 'ptr' property (available for all dynamic arrays, not just strings)\\nstr.ptr\\n\\nHowever, if you are wanting something to use with C, to ensure it is nul-terminated, use toStringz\\nimport std.string;\\ntoStringz(str);\\n\\ntoStringz will not perform a copy if the string is already nul terminated. \\n\", '6556872': \"I asked around on IRC, and as far as we can figure out it was never implemented for D1, so we're guessing it's still unimplemented. Furthermore, there is no mention of the feature in The D Programming Language, so the whole thing is up in the air a bit.\\nIf I were you, I would submit a bug against the documentation. \\n\"}}\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of nodes in G: {G.number_of_nodes()}')\n",
    "print(f'Number of edges in G: {G.number_of_edges()}')\n",
    "print(f'Edges for node 61 in G: {G.edges(str(61))}')\n",
    "\n",
    "print(f'Info about Node 61 in G: {G.nodes[str(61)]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below would find username for all users in dataset. Due to Quotas it is not possible however, so after we find top authors in our best community-split, we can request some of the top author names using stackexchange API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "break for test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Since bandwith/quota is not very little compared to dataset size, only top author names are found instead of whole dataset. This code should be applied on top authors only then.\n",
    "import requests\n",
    "import time\n",
    "remaining_users = []\n",
    "username_dict = {}\n",
    "for user_index in range(0,len(unique_user_ids),100):\n",
    "    unsuccesful_tries = 0\n",
    "    url = f'https://api.stackexchange.com/2.3/users/{\";\".join(map(str, unique_user_ids[user_index:user_index + 100]))}?site=meta.stackoverflow'\n",
    "    #url = f'https://api.stackexchange.com/2.3/users/{61}?site=meta.stackoverflow'\n",
    "    response = requests.get(url)\n",
    "    while response.status_code != 200:\n",
    "        print(f'Failed with status code: {response.status_code}')\n",
    "        time.sleep(5)\n",
    "        if unsuccesful_tries == 3:\n",
    "            remaining_users.append(unique_user_ids[user_index:user_index + 100])\n",
    "            continue\n",
    "    \n",
    "    #data = response.json()['items'][0]\n",
    "    #username_dict[str(user)] = data['display_name']\n",
    "    print('break for test')\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "source": [
    "set1 = set(np.asarray(clean_answers_df.get('Id'),dtype=str))\n",
    "set2 = set(np.asarray(clean_questions_df.get('Id'),dtype=str))\n",
    "#Id is unique in whole dataset/on stackoverflow page - is not shared between answers & questions.\n",
    "set1.intersection(set2)\n"
   ]
  }
 ]
}