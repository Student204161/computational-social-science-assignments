{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Part 2 "
      ],
      "metadata": {
        "id": "kThJZ2iXgHiE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLZb3E4ae71f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "import time \n",
        "import urllib3, socket\n",
        "from urllib3.connection import HTTPConnection\n",
        "\n",
        "\n",
        "RESPONSE_SAVE_LOC = r'C:\\Users\\khali\\OneDrive - Danmarks Tekniske Universitet\\6. semester\\Computational Social Science\\Computational-Social-Science-Exercises\\week2'\n",
        "ids_list = list(np.load(r'C:\\Users\\khali\\OneDrive - Danmarks Tekniske Universitet\\6. semester\\Computational Social Science\\Computational-Social-Science-Exercises\\week2\\remaining_ids.npy',allow_pickle=True))\n",
        "\n",
        "#Uses checkpoints in case connection to server is unstable.\n",
        "if os.path.exists(fr'{RESPONSE_SAVE_LOC}/response.json'):\n",
        "    with open(fr'{RESPONSE_SAVE_LOC}/response.json', \"r\") as f:\n",
        "        # Load the contents of the file into a Python list\n",
        "        response = json.load(f)\n",
        "    response_items = []\n",
        "    for item in response:\n",
        "        if isinstance(item,dict):\n",
        "            response_items.append(item)\n",
        "    logged_ids = [ x['authorId'] for x in response_items]\n",
        "else: response, logged_ids = [], []\n",
        "\n",
        "#Converts all ids to int and removes duplicate requests\n",
        "ids_list = [str(x) for x in ids_list]\n",
        "logged_ids = [str(x) for x in logged_ids]\n",
        "remaining_ids = list(set(ids_list) - set(logged_ids))\n",
        "\n",
        "#use first half of remaining_ids:\n",
        "remaining_ids = remaining_ids[0:int(len(remaining_ids)/2)]\n",
        "\n",
        "#np.save(fr'{RESPONSE_SAVE_LOC}/remaining_ids.npy', remaining_ids)\n",
        "\n",
        "\n",
        "#Get a list corresponding to second half of remaining_ids using length of remaining_ids:\n",
        "\n",
        "\n",
        "#del ids_list, logged_ids, response_items\n",
        "\n",
        "BASE_URL = 'https://api.semanticscholar.org/graph/'\n",
        "VERSION = 'v1/'\n",
        "RESOURCE = 'author/batch'\n",
        "CHUNK_SIZE = 100\n",
        "\n",
        "COMPLETE_URL = BASE_URL + VERSION + RESOURCE\n",
        "\n",
        "def get_request(url,json=None, params=None):\n",
        "    #Function to recursisevely call the API until we get a proper response.\n",
        "    #time.sleep(2)\n",
        "    responsex = requests.post(url, json=json, params=params, timeout=300)\n",
        "    if responsex.status_code != 200:\n",
        "        print(f'Error: {responsex.status_code}')\n",
        "        print(f'Error: {responsex.json()}')\n",
        "        #wait 5 seconds and try again\n",
        "        #time.sleep(5)\n",
        "        return get_request(url,json=json, params=params)\n",
        "    else:\n",
        "        return responsex\n",
        "\n",
        "\n",
        "#When we have to send data ind, we use requests.post() and max author request size is 100 at a time.\n",
        "params = {\"fields\": [\n",
        "    \"name,aliases,papers,papers.title,papers.abstract,papers.year,papers.externalIds,papers.s2FieldsOfStudy,papers.citationCount\"]\n",
        "}\n",
        "print(f'Retrieving data, num requests:{len(remaining_ids)}')\n",
        "for i in range(0, int(len(remaining_ids)), CHUNK_SIZE): # len(ids_list)\n",
        "    idsx = remaining_ids[i:i + CHUNK_SIZE]\n",
        "    idsx_json = {\"ids\" : idsx}\n",
        "    responsex = get_request(COMPLETE_URL, json=idsx_json, params=params).json()\n",
        "\n",
        "    if response == []:\n",
        "        response = responsex\n",
        "    else: \n",
        "        response += responsex\n",
        "    print(f'{i+CHUNK_SIZE} requests')\n",
        "\n",
        "    if i % (2*CHUNK_SIZE) == 0:\n",
        "        json_string = json.dumps(response)\n",
        "\n",
        "        with open(fr'{RESPONSE_SAVE_LOC}/response.json', \"w\") as f:\n",
        "            f.write(json_string)\n",
        "\n",
        "print('Data retrieved')\n",
        "\n",
        "print(fr'RESPONSE saved to path:{RESPONSE_SAVE_LOC}/response.json')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After saving response, use it to make dataframes."
      ],
      "metadata": {
        "id": "WmIsvusCfeAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "RESPONSE_SAVE_LOC = '/work3/s204161'\n",
        "\n",
        "#ids_list = list(np.load(r'/zhome/a7/0/155527/Desktop/s204161/Computational-Social-Science-Exercises/week2/author_ids.npy',allow_pickle=True))\n",
        "\n",
        "with open(fr'{RESPONSE_SAVE_LOC}/response.json', \"r\") as f:\n",
        "    # Load the contents of the file into a Python list\n",
        "    response = json.load(f)\n",
        "\n",
        "\n",
        "#-------------author_dataset----------------\n",
        "author_dataset_dict_list = [] #[None for x in range(len(response))]\n",
        "for counter, author in enumerate(response):\n",
        "    paper_category_list = []\n",
        "    citationCount = 0 #num times author was cited\n",
        "\n",
        "    for paper in author.get('papers'):\n",
        "        None if (paper['s2FieldsOfStudy'] == []) else (paper_category_list.append(paper['s2FieldsOfStudy'][0]['category']))\n",
        "        citationCount += paper['citationCount']\n",
        "\n",
        "    author_dataset_dict = {\n",
        "        'authorId': author['authorId'],\n",
        "        'name': author['name'],\n",
        "        'aliases': author['aliases'],\n",
        "        'citationCount': citationCount,\n",
        "        'field': None if paper_category_list == [] else max(set(paper_category_list), key=paper_category_list.count)\n",
        "\n",
        "    }\n",
        "    author_dataset_dict_list.append(author_dataset_dict)\n",
        "\n",
        "author_df = pd.DataFrame(author_dataset_dict_list)\n",
        "#print(author_df)\n",
        "\n",
        "\n",
        "#-----------------------paper dataset----------------\n",
        "\n",
        "paper_dataset_dict_list = [] #[None for x in range(len(response))]\n",
        "paper_id_dict = {}\n",
        "for counter, author in enumerate(response):\n",
        "\n",
        "    for paper in author.get('papers'):\n",
        "        if paper['paperId'] not in paper_id_dict: # Only add paper if paper not already denoted using  other author.\n",
        "            paper_id_dict[paper['paperId']] = len(paper_id_dict) #  Keep track by denoting the index of each paper in the data, chronologically.\n",
        "\n",
        "            paper_dataset_dict = {\n",
        "                    'paperId': paper['paperId'],\n",
        "                    'title': paper['title'],\n",
        "                    'year': paper['year'],\n",
        "                    'externalId.DOI': paper['externalIds'],\n",
        "                    'citationCount': paper['citationCount'], #num times \n",
        "                    'authorIds': [author['authorId']] ,\n",
        "                    's2FieldsOfStudy': None if (paper['s2FieldsOfStudy'] == []) else (paper['s2FieldsOfStudy'][0]['category'])\n",
        "                }\n",
        "            paper_dataset_dict_list.append(paper_dataset_dict)\n",
        "        else:\n",
        "            current_paper_index = paper_id_dict[paper['paperId']]\n",
        "            paper_dataset_dict_list[current_paper_index]['authorIds'].append(author['authorId'])\n",
        "\n",
        "paper_df = pd.DataFrame(paper_dataset_dict_list)\n",
        "\n",
        "\n",
        "## Paper abstract dataset.\n",
        "paper_abstract_dataset_dict_list = [] \n",
        "paper_abstract_id_dict = {}\n",
        "for counter, author in enumerate(response):\n",
        "\n",
        "    #     ##this block might be superfluos if response not faulty.\n",
        "    # if isinstance(author,str) == True:\n",
        "    #     continue\n",
        "    # elif author == None:\n",
        "    #     #print(f'Entry {counter} is None.')\n",
        "    #     continue\n",
        "    \n",
        "    for paper in author.get('papers'):\n",
        "        if paper['paperId'] not in paper_abstract_id_dict:\n",
        "            paper_abstract_id_dict[paper['paperId']] = len(paper_abstract_id_dict) #  Denote the index of each paper in the data, chronologically.\n",
        "\n",
        "            paper_abstract_dataset_dict = {\n",
        "                    'paperId': paper['paperId'],\n",
        "                    'abstract': paper['abstract'],\n",
        "                }\n",
        "            paper_abstract_dataset_dict_list.append(paper_abstract_dataset_dict)\n",
        "\n",
        "\n",
        "paper_abstract_df = pd.DataFrame(paper_abstract_dataset_dict_list)\n",
        "\n",
        "#print(paper_df)\n",
        "\n",
        "author_df.to_csv(r'/work3/s204161/author_df.csv')\n",
        "paper_df.to_csv(r'/work3/s204161/paper_df.csv')\n",
        "paper_abstract_df.to_csv(r'/work3/s204161/paper_astract_df.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "V1tSuEnVfkwX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}